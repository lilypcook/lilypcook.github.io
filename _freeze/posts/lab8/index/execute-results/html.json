{
  "hash": "dbdec5e9d148f2ec5f5c43bd7190e1e1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lab 8\"\ndescription: \"Web Scraping\"\nauthor:\n  - name: Lily Cook\ndate: 05-27-2025\ncategories: [Advanced R] \n#image: ../../media/argyle.jpg\ndraft: false \nexecute: \n  echo: true\n  warning: false\nformat: html\nembed-resources: true\neditor: source\n---\n\n\n\n\n **Goal:** Scrape information from <https://www.cheese.com> to obtain a dataset of characteristics about different cheeses, and gain deeper insight into your coding process. ðŸª¤\n\n**Part 1:** Locate and examine the `robots.txt` file for this website. Summarize what you learn from it.\n\nThe robots.txt is located at <https://www.cheese.com/robots.txt> . This file is pretty blank, with only two lines. These two lines tell us that anyone can crawl (`User-agent` is set to \\*) and where the site map is located (<https://www.cheese.com/sitemap.xml>)\n\n**Part 2:** Learn about the `html_attr()` function from `rvest`. Describe how this function works with a small example.\n\nThe `html_attr()` function gets and returns a character vector containing the values of the specified attribute for an element of the HTML file (ex. href, class, id). This function requires two arguments; `x`, the HTML element (usually from `read_html()`), `name`, the name of the attribute to retrieve, like `href`, `class`, etc. There is a third optional argument, `default`, which is related to how non-existent attributes are handled, which defaults to `NA_character`, meaning NA is returned in these cases.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define url\ncheese_url <- \"https://www.cheese.com\"\n\n# fetch html content \nhtml_doc_cheese <- read_html(cheese_url) # note: in the real world, add error handling\n\n# select all <a> tags on page\nall_links_nodes <- html_nodes(html_doc_cheese, \"a\")\n\n# use html_attr() to extract the 'href' attribute from each link\nall_hrefs_cheese <- html_attr(all_links_nodes, \"href\")\n\nhead(all_hrefs_cheese, 10)\n```\n:::\n\n\n\n\n**Part 3:** (Do this alongside Part 4 below.) I used [ChatGPT](https://chat.openai.com/chat) to start the process of scraping cheese information with the following prompt:\n\n> Write R code using the rvest package that allows me to scrape cheese information from cheese.com.\n\nFully document your process of checking this code. Record any observations you make about where ChatGPT is useful / not useful.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load libraries\nlibrary(rvest)\nlibrary(dplyr)\n\n# define url\nurl <- \"https://www.cheese.com/alphabetical\"\n\n# read html content from the page\nwebpage <- read_html(url)\n\n# extract cheese names and urls\ncheese_data <- webpage %>%\n  html_nodes(\".cheese-item\") %>%\n  html_nodes(\"a\") %>%\n  html_attr(\"href\") %>%\n  paste0(\"https://cheese.com\", .)\n\ncheese_names <- webpage %>%\n  html_nodes(\".cheese-item h3\") %>%\n  html_text()\n\n# create df to store results\ncheese_df <- data.frame(Name = cheese_names,\n                        URL = cheese_data,\n                        stringsAsFactors = FALSE)\n\nprint(cheese_df)\n```\n:::\n\n\n\n\n**Not useful:**\n\nThere were many empty results. Both cheese_data (for URLs) and cheese_names (for cheese names) were empty character vectors. Because of this, the cheese_df dataframe was also empty.\n\nThe selectors were not specific enough. The CSS selectors `.cheese-item` and `.cheese-item h3` suggested by ChatGPT were too generic/ did not accurately reflect the current structure of the cheese.com/alphabetical page. Websites frequently update their structure, and AI might be trained on old versions or make incorrect assumptions about common class names.\n\n**Useful:\\\n**ChatGPT provided a basic template of rvest functions (read_html, html_nodes, html_attr, html_text) which was conceptually helpful for recalling the workflow.\n\n**Part 4:** Obtain the following information for **all** cheeses in the database:\n\n-   cheese name\n-   URL for the cheese's webpage (e.g., <https://www.cheese.com/gouda/>)\n-   whether or not the cheese has a picture (e.g., [gouda](https://www.cheese.com/gouda/) has a picture, but [bianco](https://www.cheese.com/bianco/) does not).\n\nTo be kind to the website owners, please add a 1 second pause between page queries. (Note that you can view 100 cheeses at a time.)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(stringr)\n\n# defining url and pages to look at\nbase_url <- \"https://www.cheese.com/alphabetical/?per_page=100\"\npage_numbers <- 1:21\n\n# Helper function to extract text or attributes based on tag structure\nextract_info <- function(page, outer_selector, inner_selector, attr = NULL) {\n  nodes <- page %>%\n    html_elements(outer_selector) %>%\n    html_elements(inner_selector)\n  \n  if (!is.null(attr)) { #dealing with null values\n    html_attr(nodes, attr)\n  } else {\n    html_text(nodes)\n  }\n}\n\n# Function to scrape a single page\nscrape_cheese_page <- function(page_number) {\n  full_url <- paste0(base_url, \"&page=\", page_number)\n  page <- read_html(full_url)\n\n  data.frame(\n    Name = extract_info(page, \"div.product-item\", \"h3\"), #cheese name\n    url = paste0(\"https://www.cheese.com\", extract_info(page, \"div.product-item\", \"h3 a\", \"href\")), #cheese url\n    whether = extract_info(page, \"div.product-item\", \"img\", \"class\"), #if there is image\n    stringsAsFactors = FALSE\n  )\n}\n\n# Map over all pages and bind results\ncheese_data <- map_dfr(page_numbers, function(pg) {\n  result <- scrape_cheese_page(pg)\n  Sys.sleep(1)  # delay to be nice\n  result\n})\n\nhead(cheese_data)\n```\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}