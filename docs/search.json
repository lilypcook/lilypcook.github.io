[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am seeking employment as a data analyst, preferably in a research or higher education setting.\nI have two years of experience in a research setting, researching statistical education. This includes a culminating thesis as a requirement towards my Masters degree.\nBelow is my resume.\n\n&lt;p&gt;\nYour browser does not support PDFs. &lt;a href=\"resume.pdf\"&gt;Download the PDF&lt;/a&gt;.\n&lt;/p&gt;"
  },
  {
    "objectID": "BlogPosts.html",
    "href": "BlogPosts.html",
    "title": "Blog Posts",
    "section": "",
    "text": "These posts are created for Advanced R (STAT 541) at Cal Poly\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinal Grade Reflection\n\n\n\nAdvanced R\n\n\n\nSpring 2025\n\n\n\nLily Cook\n\n\nJun 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 9\n\n\n\nAdvanced R\n\n\n\nGenerative Art\n\n\n\nLily Cook\n\n\nJun 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 8\n\n\n\nAdvanced R\n\n\n\nWeb Scraping\n\n\n\nLily Cook\n\n\nMay 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 7\n\n\n\nAdvanced R\n\n\n\nUsing APIs\n\n\n\nLily Cook\n\n\nMay 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 6 - Spicy\n\n\n\nAdvanced R\n\n\n\nPerforming Many Different Versions of an Analysis\n\n\n\nLily Cook\n\n\nMay 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 4\n\n\n\nAdvanced R\n\n\n\nDynamic Quarto Dashboard\n\n\n\nLily Cook\n\n\nApr 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 3\n\n\n\nAdvanced R\n\n\n\nStatic Quarto Dashboard\n\n\n\nLily Cook\n\n\nApr 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 2\n\n\n\nAdvanced R\n\n\n\nAdvanced Visualizations\n\n\n\nLily Cook\n\n\nApr 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 1\n\n\n\nAdvanced R\n\n\n\nReviewing Quarto, ggplot and dplyr\n\n\n\nLily Cook\n\n\nApr 7, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/lab2/index.knit.html",
    "href": "posts/lab2/index.knit.html",
    "title": "Lab 2",
    "section": "",
    "text": "If you happen to be bored and looking for a sensible chuckle, you should check out these Bad Visualisations. Looking through these is also a good exercise in cataloging what makes a visualization good or bad.\n\n\nBelow is an example of a less-than-ideal visualization from the collection linked above. It comes to us from data provided for the Wellcome Global Monitor 2018 report by the Gallup World Poll:\n\n\nWhile there are certainly issues with this image, do your best to tell the story of this graph in words. That is, what is this graph telling you? What do you think the authors meant to convey with it?\nThis graph tells us about the belief in safety of vaccines in countries by regions of the world. The vertical ordering of the graph seems to tell a story that certain regions have higher beliefs, but this is not true.\nList the variables that appear to be displayed in this visualization. Hint: Variables refer to columns in the data.\nRegion, country, regional median belief in safety of vaccines (%), country belief in safety of vaccines (%)\nNow that you’re versed in the grammar of graphics (e.g., ggplot), list the aesthetics used and which variables are mapped to each.\ny= country belief in safety of vaccines (%), color = region, x = country\nWhat type of graph would you call this? Meaning, what geom would you use to produce this plot?\nscatterplot (geom_point), lines (probably geom_abline), text (geom_text or annotate())\nProvide at least four problems or changes that would improve this graph. Please format your changes as bullet points!\n\nChange layout of regions. The graph is too tall. Maybe make a grid of regions, or combine onto one graph with region as colors depending on density.\nChange coloring of regions. There is red and green on the graph and they are right next to each other in the original graph too.\nRemove legend, the regions are labeled on the graph.\nLabel fewer (or no) countries.\ny-axis doesn’t make sense\n\n\n\n\n\nThe data for the Wellcome Global Monitor 2018 report can be downloaded at the following site: https://wellcome.ac.uk/reports/wellcome-global-monitor/2018\n\nThere are two worksheets in the downloaded dataset file. You may need to read them in separately, but you may also just use one if it suffices.\n\nlibrary(tidyverse)\nlibrary(readxl)\n\ndata.summary &lt;- read_xlsx(\"wgm2018-dataset-crosstabs-all-countries.xlsx\", sheet = 1)\ndata.full &lt;- read_xlsx(\"wgm2018-dataset-crosstabs-all-countries.xlsx\", sheet = 2)\ndata.dictionary &lt;- read_xlsx(\"wgm2018-dataset-crosstabs-all-countries.xlsx\", sheet = 3)\n\ncountry.names &lt;- '1=United States, 2=Egypt, 3=Morocco, 4=Lebanon, 5=Saudi Arabia, 6=Jordan, 8=Turkey, 9=Pakistan, 10=Indonesia, 11=Bangladesh, 12=United Kingdom, 13=France, 14=Germany, 15=Netherlands, 16=Belgium, 17=Spain, 18=Italy, 19=Poland, 20=Hungary, 21=Czech Republic, 22=Romania, 23=Sweden, 24=Greece, 25=Denmark, 26=Iran, 28=Singapore, 29=Japan, 30=China, 31=India, 32=Venezuela, 33=Brazil, 34=Mexico, 35=Nigeria, 36=Kenya, 37=Tanzania, 38=Israel, 39=Palestinian Territories, 40=Ghana, 41=Uganda, 42=Benin, 43=Madagascar, 44=Malawi, 45=South Africa, 46=Canada, 47=Australia, 48=Philippines, 49=Sri Lanka, 50=Vietnam, 51=Thailand, 52=Cambodia, 53=Laos, 54=Myanmar, 55=New Zealand, 57=Botswana, 60=Ethiopia, 61=Mali, 62=Mauritania, 63=Mozambique, 64=Niger, 65=Rwanda, 66=Senegal, 67=Zambia, 68=South Korea, 69=Taiwan, 70=Afghanistan, 71=Belarus, 72=Georgia, 73=Kazakhstan, 74=Kyrgyzstan, 75=Moldova, 76=Russia, 77=Ukraine, 78=Burkina Faso, 79=Cameroon, 80=Sierra Leone, 81=Zimbabwe, 82=Costa Rica, 83=Albania, 84=Algeria, 87=Argentina, 88=Armenia, 89=Austria, 90=Azerbaijan, 96=Bolivia, 97=Bosnia and Herzegovina, 99=Bulgaria, 100=Burundi, 103=Chad, 104=Chile, 105=Colombia, 106=Comoros, 108=Republic of Congo, 109=Croatia, 111=Cyprus, 114=Dominican Republic, 115=Ecuador, 116=El Salvador, 119=Estonia, 121=Finland, 122=Gabon, 124=Guatemala, 125=Guinea, 128=Haiti, 129=Honduras, 130=Iceland, 131=Iraq, 132=Ireland, 134=Ivory Coast, 137=Kuwait, 138=Latvia, 140=Liberia, 141=Libya, 143=Lithuania, 144=Luxembourg, 145=Macedonia, 146=Malaysia, 148=Malta, 150=Mauritius, 153=Mongolia, 154=Montenegro, 155=Namibia, 157=Nepal, 158=Nicaragua, 160=Norway, 163=Panama, 164=Paraguay, 165=Peru, 166=Portugal, 173=Serbia, 175=Slovakia, 176=Slovenia, 183=Eswatini, 184=Switzerland, 185=Tajikistan, 186=The Gambia, 187=Togo, 190=Tunisia, 191=Turkmenistan, 193=United Arab Emirates, 194=Uruguay, 195=Uzbekistan, 197=Yemen, 198=Kosovo, 202=Northern Cyprus' \n\ncountry.names &lt;- strsplit(country.names, split = \",\")\ncountry.names &lt;- country.names[[1]]\ncountry.names &lt;- as_tibble(country.names)\ncountry.names &lt;- separate_wider_delim(country.names, \n                                      delim = \"=\",\n                                      cols = c(\"value\"), \n                                      names = c(\"WP5\", \"country\"))\n\ncountry.names &lt;- country.names %&gt;% mutate(WP5 = as.numeric(WP5))\ndata.full &lt;- merge(data.full, country.names, by = \"WP5\")\n\nregion.names &lt;- \"0=Not assigned, 1=Eastern Africa,2=Central Africa,3=North Africa,4=Southern Africa,5=Western Africa,6=Central America and Mexico,7=Northern America,8=South America,9=Central Asia,10=East Asia,11=Southeast Asia,12=South Asia,13=Middle East,14=Eastern Europe,15=Northern Europe,16=Southern Europe,17=Western Europe,18=Aus/NZ\"\n\nregion.names &lt;- strsplit(region.names, split = \",\")\nregion.names &lt;- region.names[[1]]\nregion.names &lt;- as_tibble(region.names)\nregion.names &lt;- separate_wider_delim(region.names, \n                                     delim = \"=\", \n                                     cols = c(\"value\"), \n                                     names = c(\"Regions_Report\", \"Region_Name\"))\n\nregion.names &lt;- region.names %&gt;% mutate(Regions_Report = as.numeric(Regions_Report))\ndata.full &lt;- merge(data.full, region.names, by = \"Regions_Report\")\n\ndata.full &lt;- data.full %&gt;% mutate(region = case_when(\n    Regions_Report %in% c(1, 2, 4, 5) ~ \"Sub-Saharan Africa\",\n    Regions_Report %in% c(6, 7, 8) ~ \"Americas\",\n    Regions_Report %in% c(9, 10, 11, 12, 18) ~ \"Asia\",\n    Regions_Report %in% c(3, 13) ~ \"Middle East and North Africa\",\n    Regions_Report %in% c(15, 16, 17, 0) ~ \"Europe\",\n    Regions_Report %in% c(14) ~ \"Former Soviet Union\"\n  ))\n\n\nImprove the visualization above by either re-creating it with the issues you identified fixed OR by creating a new visualization that you believe tells the same story better.\n\n\npercents &lt;- data.full %&gt;% select(country, region, Q25) %&gt;% \n  drop_na() %&gt;%\n  mutate(agree = ifelse(Q25 &lt; 3, 1, 0)) %&gt;% \n  group_by(country, region) %&gt;%\n  summarise(totagree = sum(agree), \n            tot = n(), \n            pctagree = totagree/tot)\n\nmedians &lt;- percents %&gt;%\n  group_by(region)%&gt;%\n  summarise(medregion = median(pctagree))\n\n\n\npercents %&gt;%\n    ggplot() +\n    geom_vline(data = medians, aes(xintercept = medregion), color = \"black\")+\n    geom_dotplot(aes(x = pctagree, fill = region), \n                 method = \"histodot\", \n                 dotsize = 0.6, \n                 binwidth = 0.01) +\n    facet_wrap(~region, ncol = 1)+\n    geom_text(aes(x = 0.3, y = 0.5, label = region, color = region), \n              hjust = \"inward\", \n              size =5, \n              family = \"Times\",\n              fontface = \"bold\")+\n    scale_fill_brewer(palette = \"Set2\") +\n    scale_color_brewer(palette = \"Set2\")+\n    theme_minimal() +\n    theme(legend.position = \"none\",\n      panel.grid.major.y = element_blank(),\n      panel.grid.minor.y = element_blank(),\n      axis.text.y = element_blank(),\n      strip.text = element_blank(),\n      text= element_text(family = \"Times\"),\n      plot.title = element_text(face = \"bold\")) +\n      scale_x_continuous(labels = scales::percent_format(accuracy = 1))+\n      labs(title = \"Percent of people who believe vaccines are safe by country and global region\", subtitle = \"Black lines represent region median\", \n           x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nI decided to create a dotplot to better show the distributions of the regions, and make better use of the y-axis. I also removed labels because they were distracting and messy, and did not include all countries."
  },
  {
    "objectID": "posts/lab2/index.knit.html#dissecting-a-bad-visualization",
    "href": "posts/lab2/index.knit.html#dissecting-a-bad-visualization",
    "title": "Lab 2",
    "section": "",
    "text": "Below is an example of a less-than-ideal visualization from the collection linked above. It comes to us from data provided for the Wellcome Global Monitor 2018 report by the Gallup World Poll:\n\n\nWhile there are certainly issues with this image, do your best to tell the story of this graph in words. That is, what is this graph telling you? What do you think the authors meant to convey with it?\nThis graph tells us about the belief in safety of vaccines in countries by regions of the world. The vertical ordering of the graph seems to tell a story that certain regions have higher beliefs, but this is not true.\nList the variables that appear to be displayed in this visualization. Hint: Variables refer to columns in the data.\nRegion, country, regional median belief in safety of vaccines (%), country belief in safety of vaccines (%)\nNow that you’re versed in the grammar of graphics (e.g., ggplot), list the aesthetics used and which variables are mapped to each.\ny= country belief in safety of vaccines (%), color = region, x = country\nWhat type of graph would you call this? Meaning, what geom would you use to produce this plot?\nscatterplot (geom_point), lines (probably geom_abline), text (geom_text or annotate())\nProvide at least four problems or changes that would improve this graph. Please format your changes as bullet points!\n\nChange layout of regions. The graph is too tall. Maybe make a grid of regions, or combine onto one graph with region as colors depending on density.\nChange coloring of regions. There is red and green on the graph and they are right next to each other in the original graph too.\nRemove legend, the regions are labeled on the graph.\nLabel fewer (or no) countries.\ny-axis doesn’t make sense"
  },
  {
    "objectID": "posts/lab2/index.knit.html#improving-the-bad-visualization",
    "href": "posts/lab2/index.knit.html#improving-the-bad-visualization",
    "title": "Lab 2",
    "section": "",
    "text": "The data for the Wellcome Global Monitor 2018 report can be downloaded at the following site: https://wellcome.ac.uk/reports/wellcome-global-monitor/2018\n\nThere are two worksheets in the downloaded dataset file. You may need to read them in separately, but you may also just use one if it suffices.\n\nlibrary(tidyverse)\nlibrary(readxl)\n\ndata.summary &lt;- read_xlsx(\"wgm2018-dataset-crosstabs-all-countries.xlsx\", sheet = 1)\ndata.full &lt;- read_xlsx(\"wgm2018-dataset-crosstabs-all-countries.xlsx\", sheet = 2)\ndata.dictionary &lt;- read_xlsx(\"wgm2018-dataset-crosstabs-all-countries.xlsx\", sheet = 3)\n\ncountry.names &lt;- '1=United States, 2=Egypt, 3=Morocco, 4=Lebanon, 5=Saudi Arabia, 6=Jordan, 8=Turkey, 9=Pakistan, 10=Indonesia, 11=Bangladesh, 12=United Kingdom, 13=France, 14=Germany, 15=Netherlands, 16=Belgium, 17=Spain, 18=Italy, 19=Poland, 20=Hungary, 21=Czech Republic, 22=Romania, 23=Sweden, 24=Greece, 25=Denmark, 26=Iran, 28=Singapore, 29=Japan, 30=China, 31=India, 32=Venezuela, 33=Brazil, 34=Mexico, 35=Nigeria, 36=Kenya, 37=Tanzania, 38=Israel, 39=Palestinian Territories, 40=Ghana, 41=Uganda, 42=Benin, 43=Madagascar, 44=Malawi, 45=South Africa, 46=Canada, 47=Australia, 48=Philippines, 49=Sri Lanka, 50=Vietnam, 51=Thailand, 52=Cambodia, 53=Laos, 54=Myanmar, 55=New Zealand, 57=Botswana, 60=Ethiopia, 61=Mali, 62=Mauritania, 63=Mozambique, 64=Niger, 65=Rwanda, 66=Senegal, 67=Zambia, 68=South Korea, 69=Taiwan, 70=Afghanistan, 71=Belarus, 72=Georgia, 73=Kazakhstan, 74=Kyrgyzstan, 75=Moldova, 76=Russia, 77=Ukraine, 78=Burkina Faso, 79=Cameroon, 80=Sierra Leone, 81=Zimbabwe, 82=Costa Rica, 83=Albania, 84=Algeria, 87=Argentina, 88=Armenia, 89=Austria, 90=Azerbaijan, 96=Bolivia, 97=Bosnia and Herzegovina, 99=Bulgaria, 100=Burundi, 103=Chad, 104=Chile, 105=Colombia, 106=Comoros, 108=Republic of Congo, 109=Croatia, 111=Cyprus, 114=Dominican Republic, 115=Ecuador, 116=El Salvador, 119=Estonia, 121=Finland, 122=Gabon, 124=Guatemala, 125=Guinea, 128=Haiti, 129=Honduras, 130=Iceland, 131=Iraq, 132=Ireland, 134=Ivory Coast, 137=Kuwait, 138=Latvia, 140=Liberia, 141=Libya, 143=Lithuania, 144=Luxembourg, 145=Macedonia, 146=Malaysia, 148=Malta, 150=Mauritius, 153=Mongolia, 154=Montenegro, 155=Namibia, 157=Nepal, 158=Nicaragua, 160=Norway, 163=Panama, 164=Paraguay, 165=Peru, 166=Portugal, 173=Serbia, 175=Slovakia, 176=Slovenia, 183=Eswatini, 184=Switzerland, 185=Tajikistan, 186=The Gambia, 187=Togo, 190=Tunisia, 191=Turkmenistan, 193=United Arab Emirates, 194=Uruguay, 195=Uzbekistan, 197=Yemen, 198=Kosovo, 202=Northern Cyprus' \n\ncountry.names &lt;- strsplit(country.names, split = \",\")\ncountry.names &lt;- country.names[[1]]\ncountry.names &lt;- as_tibble(country.names)\ncountry.names &lt;- separate_wider_delim(country.names, \n                                      delim = \"=\",\n                                      cols = c(\"value\"), \n                                      names = c(\"WP5\", \"country\"))\n\ncountry.names &lt;- country.names %&gt;% mutate(WP5 = as.numeric(WP5))\ndata.full &lt;- merge(data.full, country.names, by = \"WP5\")\n\nregion.names &lt;- \"0=Not assigned, 1=Eastern Africa,2=Central Africa,3=North Africa,4=Southern Africa,5=Western Africa,6=Central America and Mexico,7=Northern America,8=South America,9=Central Asia,10=East Asia,11=Southeast Asia,12=South Asia,13=Middle East,14=Eastern Europe,15=Northern Europe,16=Southern Europe,17=Western Europe,18=Aus/NZ\"\n\nregion.names &lt;- strsplit(region.names, split = \",\")\nregion.names &lt;- region.names[[1]]\nregion.names &lt;- as_tibble(region.names)\nregion.names &lt;- separate_wider_delim(region.names, \n                                     delim = \"=\", \n                                     cols = c(\"value\"), \n                                     names = c(\"Regions_Report\", \"Region_Name\"))\n\nregion.names &lt;- region.names %&gt;% mutate(Regions_Report = as.numeric(Regions_Report))\ndata.full &lt;- merge(data.full, region.names, by = \"Regions_Report\")\n\ndata.full &lt;- data.full %&gt;% mutate(region = case_when(\n    Regions_Report %in% c(1, 2, 4, 5) ~ \"Sub-Saharan Africa\",\n    Regions_Report %in% c(6, 7, 8) ~ \"Americas\",\n    Regions_Report %in% c(9, 10, 11, 12, 18) ~ \"Asia\",\n    Regions_Report %in% c(3, 13) ~ \"Middle East and North Africa\",\n    Regions_Report %in% c(15, 16, 17, 0) ~ \"Europe\",\n    Regions_Report %in% c(14) ~ \"Former Soviet Union\"\n  ))\n\n\nImprove the visualization above by either re-creating it with the issues you identified fixed OR by creating a new visualization that you believe tells the same story better.\n\n\npercents &lt;- data.full %&gt;% select(country, region, Q25) %&gt;% \n  drop_na() %&gt;%\n  mutate(agree = ifelse(Q25 &lt; 3, 1, 0)) %&gt;% \n  group_by(country, region) %&gt;%\n  summarise(totagree = sum(agree), \n            tot = n(), \n            pctagree = totagree/tot)\n\nmedians &lt;- percents %&gt;%\n  group_by(region)%&gt;%\n  summarise(medregion = median(pctagree))\n\n\n\npercents %&gt;%\n    ggplot() +\n    geom_vline(data = medians, aes(xintercept = medregion), color = \"black\")+\n    geom_dotplot(aes(x = pctagree, fill = region), \n                 method = \"histodot\", \n                 dotsize = 0.6, \n                 binwidth = 0.01) +\n    facet_wrap(~region, ncol = 1)+\n    geom_text(aes(x = 0.3, y = 0.5, label = region, color = region), \n              hjust = \"inward\", \n              size =5, \n              family = \"Times\",\n              fontface = \"bold\")+\n    scale_fill_brewer(palette = \"Set2\") +\n    scale_color_brewer(palette = \"Set2\")+\n    theme_minimal() +\n    theme(legend.position = \"none\",\n      panel.grid.major.y = element_blank(),\n      panel.grid.minor.y = element_blank(),\n      axis.text.y = element_blank(),\n      strip.text = element_blank(),\n      text= element_text(family = \"Times\"),\n      plot.title = element_text(face = \"bold\")) +\n      scale_x_continuous(labels = scales::percent_format(accuracy = 1))+\n      labs(title = \"Percent of people who believe vaccines are safe by country and global region\", subtitle = \"Black lines represent region median\", \n           x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nI decided to create a dotplot to better show the distributions of the regions, and make better use of the y-axis. I also removed labels because they were distracting and messy, and did not include all countries."
  },
  {
    "objectID": "posts/lab2/index.knit.html#second-data-visualization-improvement",
    "href": "posts/lab2/index.knit.html#second-data-visualization-improvement",
    "title": "Lab 2",
    "section": "Second Data Visualization Improvement",
    "text": "Second Data Visualization Improvement\nFor this second plot, you must select a plot that uses maps so you can demonstrate your proficiency with the leaflet package!\n\nSelect a data visualization in the report that you think could be improved. Be sure to cite both the page number and figure title. Do your best to tell the story of this graph in words. That is, what is this graph telling you? What do you think the authors meant to convey with it?\nChart 5.7, page 121\n\nList the variables that appear to be displayed in this visualization.\nCountry, % of people claiming to have ever vaccinated their children (Q28)\nNow that you’re versed in the grammar of graphics (ggplot), list the aesthetics used and which variables are specified for each.\nfill = % of children vaccinated\nWhat type of graph would you call this?\nMap\nList all of the problems or things you would improve about this graph.\n\nThe less than 70% category color is very close to the country not surveyed color.\nToo much text in title/subtitle - people will not read all of that\nIn my opinion, green on a map does not necessarily convey that there is data being communicated on this graph- the color green is often associated with land in general. It could be beneficial to change the color so people don’t just scroll past this map.\n\nImprove the visualization above by either re-creating it with the issues you identified fixed OR by creating a new visualization that you believe tells the same story better.\n\n\nlibrary(leaflet)\nlibrary(forcats)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\n\nvaccinepct &lt;- data.full %&gt;% select(country, Q28) %&gt;% drop_na() %&gt;% \n  group_by(country) %&gt;% \n  summarise(tot = n(), \n            totyes = sum(Q28==1), \n            pctyes = totyes/tot) %&gt;%\n  mutate(pctgroup = case_when(\n    pctyes &lt; 0.7 ~ 1,\n    pctyes &gt;= 0.7 & pctyes &lt; 0.8 ~ 2,\n    pctyes &gt;= 0.8 & pctyes &lt; 0.9 ~ 3,\n    pctyes &gt;= 0.9 & pctyes &lt; 0.95 ~ 4,\n    pctyes &gt;= 0.95 ~ 5\n  ))\n\nmapdata &lt;-  ne_countries(scale = \"medium\", returnclass = \"sf\")\n\nvaccinepct &lt;- vaccinepct %&gt;%\n  mutate(country = case_when(\n    country == \"Eswatini\" ~ \"Kingdom of eSwatini\",\n    country == \"Ivory Coast\" ~ \"Côte d'Ivoire\",\n    country == \"Laos\" ~ \"Lao PDR\",\n    country == \"Macedonia\" ~ \"North Macedonia\",\n    country == \"Palestinian Territories\" ~ \"Palestine\",\n    country == \"Republic of Congo\" ~ \"Republic of the Congo\",\n    country == \"Russia\" ~ \"Russian Federation\",\n    country == \"South Korea\" ~ \"Republic of Korea\",\n    TRUE ~ country), \n    name_long = country)\n\nvaccinepct[which(!(vaccinepct$name_long %in% mapdata$name_long)), 1]\n\n# A tibble: 0 × 1\n# ℹ 1 variable: country &lt;chr&gt;\n\nmergedmap &lt;- left_join(mapdata, vaccinepct, by = \"name_long\")\n\nmergedmap &lt;- mergedmap %&gt;%\n  mutate(pctgroup = case_when(\n    pctyes &lt; 0.7 ~ 1,\n    pctyes &gt;= 0.7 & pctyes &lt; 0.8 ~ 2,\n    pctyes &gt;= 0.8 & pctyes &lt; 0.9 ~ 3,\n    pctyes &gt;= 0.9 & pctyes &lt; 0.95 ~ 4,\n    pctyes &gt;= 0.95 ~ 5,\n    is.na(pctyes) ~ 0),\n    percentlabel = round(pctyes * 100, 2),\n    percentlabel = ifelse(is.na(percentlabel), \"Not surveyed\", paste(percentlabel,\"%\")),\n    grouplabel = case_when(\n      pctgroup == 0 ~ \"Not surveyed\",\n      pctgroup == 1 ~ \"Less than 70%\",\n      pctgroup == 2 ~ \"70% to 79%\",\n      pctgroup == 3 ~ \"80% to 89%\",\n      pctgroup == 4 ~ \"90% to 94%\",\n      pctgroup == 5 ~ \"95% and above\"\n    ), \n    grouplabel = fct_reorder(grouplabel, pctgroup))\n\nmappal &lt;- colorNumeric('RdPu', domain = mergedmap$pctgroup)\nlabelpal &lt;- colorFactor('RdPu', domain = mergedmap$grouplabel)\n\nleaflet(mergedmap) %&gt;%\n  addTiles() %&gt;%\n  addPolygons(\n    fillColor = ~ mappal(pctgroup),\n    weight = 1, fillOpacity = 0.7,\n    color = \"lightgray\",\n    label = ~ paste(name_long, \":\", percentlabel)) %&gt;%\n  addLegend(\n    pal = labelpal, values = ~ grouplabel,  \n    title = \"People who say their children have been vaccinated\"\n  )"
  },
  {
    "objectID": "posts/lab2/index.knit.html#third-data-visualization-improvement",
    "href": "posts/lab2/index.knit.html#third-data-visualization-improvement",
    "title": "Lab 2",
    "section": "Third Data Visualization Improvement",
    "text": "Third Data Visualization Improvement\nFor this third plot, you must use one of the other ggplot2 extension packages mentioned this week (e.g., gganimate, plotly, patchwork, cowplot).\n\nSelect a data visualization in the report that you think could be improved. Be sure to cite both the page number and figure title. Do your best to tell the story of this graph in words. That is, what is this graph telling you? What do you think the authors meant to convey with it?\nChart 4.5, page 84\n\nThis chart is trying to categorize people based on their response to two of the questions. There seems to be the most people in the “Enthusiasts” and “The Included” category. Other than that I have really no idea what they are trying to say with this graph. I think it is poorly done.\nList the variables that appear to be displayed in this visualization.\nResponse type, views on extent to which science benefits society (country level), views on extent to which science benefits people normally (personal level)\nNow that you’re versed in the grammar of graphics (ggplot), list the aesthetics used and which variables are specified for each.\nThis seems to be a geom_point(), with aes(x= personal level, y = country level, fill = category, size = percentage in each category)\nWhat type of graph would you call this?\nBubble chart/ plot\nList all of the problems or things you would improve about this graph.\n\nI don’t think this is necessarily a good graph to tell the story that is trying to be told. I think a bar chart would be better, as people are not good at identifying/comparing area/size of circles.\nIf it is important to tell the story of where every category is located on the plot, a scatterplot could be good\nIt is not clear what each category corresponds to in terms of answers given.\nAxis labels :(\n\nImprove the visualization above by either re-creating it with the issues you identified fixed OR by creating a new visualization that you believe tells the same story better.\n\n\nlibrary(cowplot)\nlibrary(RColorBrewer)\n\nscience &lt;- data.full %&gt;% \n  select(Q17, Q18, ViewOfScience) %&gt;% \n  filter(ViewOfScience != 99, Q18 %in% c(1,2)) %&gt;%\n  mutate(ViewOfScience = as.factor(ViewOfScience),\n         label = case_when(\n           ViewOfScience == 1 ~ \"Enthusiast \\nYes, Yes\",\n           ViewOfScience == 2 ~ \"Included \\nYes, No\",\n           ViewOfScience == 3 ~ \"Excluded \\nNo, Yes\",\n           ViewOfScience == 4 ~ \"Sceptic \\nNo, No\"\n         ))\n\nscience_long &lt;- pivot_longer(science, cols = c(Q17, Q18), \n                            names_to = \"Q\", values_to = \"Response\") %&gt;%\n  mutate(Response = factor(Response),\n         label = ifelse(Response == 1, \"Yes\", \"No\"))\n\nstacked &lt;- ggplot(data = science, aes(x = \"\", fill = ViewOfScience))+\n  geom_bar(position = \"fill\")+\n  geom_text(aes(label = label), \n            stat = \"count\",\n            position = position_fill(vjust = 0.5),\n            family = \"Times\") +\n  scale_y_continuous(minor_breaks = NULL, \"\", \n                     labels = scales::percent_format(accuracy = 1))+\n  scale_fill_brewer(palette = \"Paired\")+\n  theme_cowplot(12)+\n  theme(text = element_text(\"Times\"), \n        legend.position = \"none\", \n        plot.title = element_text(face = \"bold\"))+\n  labs(title = \"Combined and individual views of peoples' opinions on benefits of science on a personal and country level\",\n       subtitle = \"Labels on left show overall and single views on 1) individual and 2) country level benefits of science\",\n       x = \"Combined Opinion\")\n\n\n\nsidebyside &lt;- ggplot(data = science_long,\n                    aes(x = Q, fill = Response))+\n  geom_bar(position = \"fill\")+\n  geom_text(aes(label = label), \n            stat = \"count\",\n            position = position_fill(vjust = 0.5),\n            family = \"Times\")+ \n  scale_y_continuous(breaks = NULL, \"\", \n                     labels = element_blank(),\n                     )+\n  scale_x_discrete(labels = c(\"Individual Level\", \"Country level\"))+\n  theme_cowplot(12)+\n  theme(text = element_text(\"Times\"), \n        legend.position = \"none\", \n        plot.title = element_text(face = \"bold\"))+\n  labs(title = \"\",\n       x = \"Single Opinion\") +\n  scale_fill_manual(values = brewer.pal(12, \"Paired\")[9:10])\n\nplot_grid(stacked, sidebyside)"
  },
  {
    "objectID": "posts/lab1/index.html",
    "href": "posts/lab1/index.html",
    "title": "Lab 1",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggridges)"
  },
  {
    "objectID": "posts/lab1/index.html#quarto",
    "href": "posts/lab1/index.html#quarto",
    "title": "Lab 1",
    "section": "Quarto",
    "text": "Quarto\nFirst, let’s make sure you know how to use Markdown formatting to style a Quarto document.\n\nMake this text bold.\nMake this text italicized.\nMake these into a bullet point list:\n\n\nApples\nBananas\nPotatoes\n\n\nEdit the YAML to remove warning messages from being output in the rendered HTML file\nUsing code chunk options, make it so this chunk shows the plot but not the source code:\n\n\n\n\n\n\n\n\n\n\n\nUsing code chunk options, remove the messages about bandwidth geom_density_ridges() chose to use:\n\n\nggplot(data = mpg, \n       mapping = aes(y = manufacturer, x = hwy)) + \n  geom_density_ridges() +\n  labs(x = \"\",\n       y = \"\", \n       title = \"Highway Milage (mpg) for Different Car Manufacturers\"\n       )\n\n\n\n\n\n\n\n\n\nUsing code chunk options, make it so that these plots are printed side-by-side:\n\nggplot(data = mpg, \n       mapping = aes(y = manufacturer, x = hwy)) + \n  geom_boxplot() +\n  labs(x = \"\",\n       y = \"\", \n       title = \"Highway Milage (mpg) for Different Car Manufacturers\"\n       )\nggplot(data = mpg, \n       mapping = aes(y = manufacturer, x = hwy)) + \n  geom_density_ridges() +\n  labs(x = \"\",\n       y = \"\", \n       title = \"Highway Milage (mpg) for Different Car Manufacturers\"\n       )\n\n\n\n\n\n\n\n\n\n\n\nUsing code chunk options, make it so this chunk shows the code but not the output:\n\n\n2 + 2\n\n\nUsing code chunk options, make it so the file can still knit even though this chunk has an error\n\n\n2 + a\n\n\nUsing code chunk options, create a descriptive label for each of the code chunks above."
  },
  {
    "objectID": "posts/lab1/index.html#data-wrangling-review",
    "href": "posts/lab1/index.html#data-wrangling-review",
    "title": "Lab 1",
    "section": "Data Wrangling Review",
    "text": "Data Wrangling Review\nSince you already seen some ggplots, let’s do a bit of review on data handling. In this class, we will exclusively make use of tools from the tidyverse suite of packages to perform our data cleaning and wrangling operations. If you are less familiar with these packages or it’s been some time since you used them, I would strongly recommend referencing the function documentation!\nFor these problems, we will continue to work with the mpg data frame, making various changes to the data to clean it up.\n\nThe fl variable describes the type of fuel for each car, with levels: p, r, e, d, and c. Do some research into what each of these labels mean! Then, use the if_else() function to create a new variable (fuel_type) with two levels: petrol (any car using petroleum-based gas) and alternative energy (any car not using petroleum-based gas).\n\nP: premium, R: regular, E: electric, D: diesel, C: Compressed Natural Gas (CNG) (no clear official documentation, this could be incorrect)\n\nmpg &lt;- mpg %&gt;%\n  mutate(fuel_type = if_else(fl == \"c\"|fl==\"e\", \"alternative energy\",\"petrol\"))\n\n\nThe drv variable describes if the car has front drive (f), rear drive (r), or four wheel drive (4). Let’s make better labels for these values! Specifically, use the case_when() function to change the drv variable to have the following levels: front, rear, four wheel.\n\n\nmpg &lt;- mpg %&gt;%\n  mutate(drv = case_when(\n    drv == \"f\"~\"front\",\n    drv == \"r\"~\"rear\",\n    drv == \"4\"~\"four wheel\"\n  ))\n\n13. The trans variable contains two pieces of information, (1) the transmission style (auto or manual) and the specific type of transmission (e.g., l5, m5). Using the str_split() function, create a new variable (trans_type) containing the specific type of transmission of each car. Once you’ve made this new variable, use the rename() function to change the name of the trans column to trans_style.\nHint: You will need to deal with the stray parenthesis! (string split + remove extra “)” )\n\nmpg &lt;- mpg %&gt;% \n  mutate(trans_type = str_split(trans, \"\\\\(\", simplify = T)[,2],\n         trans_type = str_remove(trans_type,\"\\\\)\"),\n         trans = str_split(trans, \"\\\\(\", simplify = T)[,1]\n         ) %&gt;%\n  rename(trans_style=trans)"
  },
  {
    "objectID": "posts/lab1/index.html#getting-to-know-your-classmates",
    "href": "posts/lab1/index.html#getting-to-know-your-classmates",
    "title": "Lab 1",
    "section": "Getting to know your classmates",
    "text": "Getting to know your classmates\n\nFind someone who took Stat 331 from a different professor than you. Compare your experiences. Tell me their name and professor. List one or two things that you think you learned more about, and one or two things that they learned more about.\nJacob (the GOAT) took STAT 331 with Dr. Theobold.\nJacob learned more about stringr. I did more work with dplyr and tidyverse as this was the main focus of the course.\nFind someone in the class who does not share your birth month. Tell me their name and birthday, and use R to find out how many days apart your birthdays are.\nRachel’s birthday is August 14, 2002. Mine is July 11, 2002.\n\nstart_date &lt;- as.Date(\"2002-07-11\")\nend_date   &lt;- as.Date(\"2002-08-14\")\nnum_days &lt;- end_date - start_date\nnum_days\n\nTime difference of 34 days"
  },
  {
    "objectID": "posts/lab2/index.html",
    "href": "posts/lab2/index.html",
    "title": "Lab 2",
    "section": "",
    "text": "If you happen to be bored and looking for a sensible chuckle, you should check out these Bad Visualisations. Looking through these is also a good exercise in cataloging what makes a visualization good or bad.\n\n\nBelow is an example of a less-than-ideal visualization from the collection linked above. It comes to us from data provided for the Wellcome Global Monitor 2018 report by the Gallup World Poll:\n\n\nWhile there are certainly issues with this image, do your best to tell the story of this graph in words. That is, what is this graph telling you? What do you think the authors meant to convey with it?\nThis graph tells us about the belief in safety of vaccines in countries by regions of the world. The vertical ordering of the graph seems to tell a story that certain regions have higher beliefs, but this is not true.\nList the variables that appear to be displayed in this visualization. Hint: Variables refer to columns in the data.\nRegion, country, regional median belief in safety of vaccines (%), country belief in safety of vaccines (%)\nNow that you’re versed in the grammar of graphics (e.g., ggplot), list the aesthetics used and which variables are mapped to each.\ny= country belief in safety of vaccines (%), color = region, x = country\nWhat type of graph would you call this? Meaning, what geom would you use to produce this plot?\nscatterplot (geom_point), lines (probably geom_abline), text (geom_text or annotate())\nProvide at least four problems or changes that would improve this graph. Please format your changes as bullet points!\n\nChange layout of regions. The graph is too tall. Maybe make a grid of regions, or combine onto one graph with region as colors depending on density.\nChange coloring of regions. There is red and green on the graph and they are right next to each other in the original graph too.\nRemove legend, the regions are labeled on the graph.\nLabel fewer (or no) countries.\ny-axis doesn’t make sense\n\n\n\n\n\nThe data for the Wellcome Global Monitor 2018 report can be downloaded at the following site: https://wellcome.ac.uk/reports/wellcome-global-monitor/2018\n\nThere are two worksheets in the downloaded dataset file. You may need to read them in separately, but you may also just use one if it suffices.\n\nlibrary(tidyverse)\nlibrary(readxl)\n\ndata.summary &lt;- read_xlsx(\"wgm2018-dataset-crosstabs-all-countries.xlsx\", sheet = 1)\ndata.full &lt;- read_xlsx(\"wgm2018-dataset-crosstabs-all-countries.xlsx\", sheet = 2)\ndata.dictionary &lt;- read_xlsx(\"wgm2018-dataset-crosstabs-all-countries.xlsx\", sheet = 3)\n\ncountry.names &lt;- '1=United States, 2=Egypt, 3=Morocco, 4=Lebanon, 5=Saudi Arabia, 6=Jordan, 8=Turkey, 9=Pakistan, 10=Indonesia, 11=Bangladesh, 12=United Kingdom, 13=France, 14=Germany, 15=Netherlands, 16=Belgium, 17=Spain, 18=Italy, 19=Poland, 20=Hungary, 21=Czech Republic, 22=Romania, 23=Sweden, 24=Greece, 25=Denmark, 26=Iran, 28=Singapore, 29=Japan, 30=China, 31=India, 32=Venezuela, 33=Brazil, 34=Mexico, 35=Nigeria, 36=Kenya, 37=Tanzania, 38=Israel, 39=Palestinian Territories, 40=Ghana, 41=Uganda, 42=Benin, 43=Madagascar, 44=Malawi, 45=South Africa, 46=Canada, 47=Australia, 48=Philippines, 49=Sri Lanka, 50=Vietnam, 51=Thailand, 52=Cambodia, 53=Laos, 54=Myanmar, 55=New Zealand, 57=Botswana, 60=Ethiopia, 61=Mali, 62=Mauritania, 63=Mozambique, 64=Niger, 65=Rwanda, 66=Senegal, 67=Zambia, 68=South Korea, 69=Taiwan, 70=Afghanistan, 71=Belarus, 72=Georgia, 73=Kazakhstan, 74=Kyrgyzstan, 75=Moldova, 76=Russia, 77=Ukraine, 78=Burkina Faso, 79=Cameroon, 80=Sierra Leone, 81=Zimbabwe, 82=Costa Rica, 83=Albania, 84=Algeria, 87=Argentina, 88=Armenia, 89=Austria, 90=Azerbaijan, 96=Bolivia, 97=Bosnia and Herzegovina, 99=Bulgaria, 100=Burundi, 103=Chad, 104=Chile, 105=Colombia, 106=Comoros, 108=Republic of Congo, 109=Croatia, 111=Cyprus, 114=Dominican Republic, 115=Ecuador, 116=El Salvador, 119=Estonia, 121=Finland, 122=Gabon, 124=Guatemala, 125=Guinea, 128=Haiti, 129=Honduras, 130=Iceland, 131=Iraq, 132=Ireland, 134=Ivory Coast, 137=Kuwait, 138=Latvia, 140=Liberia, 141=Libya, 143=Lithuania, 144=Luxembourg, 145=Macedonia, 146=Malaysia, 148=Malta, 150=Mauritius, 153=Mongolia, 154=Montenegro, 155=Namibia, 157=Nepal, 158=Nicaragua, 160=Norway, 163=Panama, 164=Paraguay, 165=Peru, 166=Portugal, 173=Serbia, 175=Slovakia, 176=Slovenia, 183=Eswatini, 184=Switzerland, 185=Tajikistan, 186=The Gambia, 187=Togo, 190=Tunisia, 191=Turkmenistan, 193=United Arab Emirates, 194=Uruguay, 195=Uzbekistan, 197=Yemen, 198=Kosovo, 202=Northern Cyprus' \n\ncountry.names &lt;- strsplit(country.names, split = \",\")\ncountry.names &lt;- country.names[[1]]\ncountry.names &lt;- as_tibble(country.names)\ncountry.names &lt;- separate_wider_delim(country.names, \n                                      delim = \"=\",\n                                      cols = c(\"value\"), \n                                      names = c(\"WP5\", \"country\"))\n\ncountry.names &lt;- country.names %&gt;% mutate(WP5 = as.numeric(WP5))\ndata.full &lt;- merge(data.full, country.names, by = \"WP5\")\n\nregion.names &lt;- \"0=Not assigned, 1=Eastern Africa,2=Central Africa,3=North Africa,4=Southern Africa,5=Western Africa,6=Central America and Mexico,7=Northern America,8=South America,9=Central Asia,10=East Asia,11=Southeast Asia,12=South Asia,13=Middle East,14=Eastern Europe,15=Northern Europe,16=Southern Europe,17=Western Europe,18=Aus/NZ\"\n\nregion.names &lt;- strsplit(region.names, split = \",\")\nregion.names &lt;- region.names[[1]]\nregion.names &lt;- as_tibble(region.names)\nregion.names &lt;- separate_wider_delim(region.names, \n                                     delim = \"=\", \n                                     cols = c(\"value\"), \n                                     names = c(\"Regions_Report\", \"Region_Name\"))\n\nregion.names &lt;- region.names %&gt;% mutate(Regions_Report = as.numeric(Regions_Report))\ndata.full &lt;- merge(data.full, region.names, by = \"Regions_Report\")\n\ndata.full &lt;- data.full %&gt;% mutate(region = case_when(\n    Regions_Report %in% c(1, 2, 4, 5) ~ \"Sub-Saharan Africa\",\n    Regions_Report %in% c(6, 7, 8) ~ \"Americas\",\n    Regions_Report %in% c(9, 10, 11, 12, 18) ~ \"Asia\",\n    Regions_Report %in% c(3, 13) ~ \"Middle East and North Africa\",\n    Regions_Report %in% c(15, 16, 17, 0) ~ \"Europe\",\n    Regions_Report %in% c(14) ~ \"Former Soviet Union\"\n  ))\n\n\nImprove the visualization above by either re-creating it with the issues you identified fixed OR by creating a new visualization that you believe tells the same story better.\n\n\npercents &lt;- data.full %&gt;% select(country, region, Q25) %&gt;% \n  drop_na() %&gt;%\n  mutate(agree = ifelse(Q25 &lt; 3, 1, 0)) %&gt;% \n  group_by(country, region) %&gt;%\n  summarise(totagree = sum(agree), \n            tot = n(), \n            pctagree = totagree/tot)\n\nmedians &lt;- percents %&gt;%\n  group_by(region)%&gt;%\n  summarise(medregion = median(pctagree))\n\n\n\npercents %&gt;%\n    ggplot() +\n    geom_vline(data = medians, aes(xintercept = medregion), color = \"black\")+\n    geom_dotplot(aes(x = pctagree, fill = region), \n                 method = \"histodot\", \n                 dotsize = 0.6, \n                 binwidth = 0.01) +\n    facet_wrap(~region, ncol = 1)+\n    geom_text(aes(x = 0.3, y = 0.5, label = region, color = region), \n              hjust = \"inward\", \n              size =5, \n              family = \"Times\",\n              fontface = \"bold\")+\n    scale_fill_brewer(palette = \"Set2\") +\n    scale_color_brewer(palette = \"Set2\")+\n    theme_minimal() +\n    theme(legend.position = \"none\",\n      panel.grid.major.y = element_blank(),\n      panel.grid.minor.y = element_blank(),\n      axis.text.y = element_blank(),\n      strip.text = element_blank(),\n      text= element_text(family = \"Times\"),\n      plot.title = element_text(face = \"bold\")) +\n      scale_x_continuous(labels = scales::percent_format(accuracy = 1))+\n      labs(title = \"Percent of people who believe vaccines are safe by country and global region\", subtitle = \"Black lines represent region median\", \n           x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nI decided to create a dotplot to better show the distributions of the regions, and make better use of the y-axis. I also removed labels because they were distracting and messy, and did not include all countries."
  },
  {
    "objectID": "posts/lab2/index.html#dissecting-a-bad-visualization",
    "href": "posts/lab2/index.html#dissecting-a-bad-visualization",
    "title": "Lab 2",
    "section": "",
    "text": "Below is an example of a less-than-ideal visualization from the collection linked above. It comes to us from data provided for the Wellcome Global Monitor 2018 report by the Gallup World Poll:\n\n\nWhile there are certainly issues with this image, do your best to tell the story of this graph in words. That is, what is this graph telling you? What do you think the authors meant to convey with it?\nThis graph tells us about the belief in safety of vaccines in countries by regions of the world. The vertical ordering of the graph seems to tell a story that certain regions have higher beliefs, but this is not true.\nList the variables that appear to be displayed in this visualization. Hint: Variables refer to columns in the data.\nRegion, country, regional median belief in safety of vaccines (%), country belief in safety of vaccines (%)\nNow that you’re versed in the grammar of graphics (e.g., ggplot), list the aesthetics used and which variables are mapped to each.\ny= country belief in safety of vaccines (%), color = region, x = country\nWhat type of graph would you call this? Meaning, what geom would you use to produce this plot?\nscatterplot (geom_point), lines (probably geom_abline), text (geom_text or annotate())\nProvide at least four problems or changes that would improve this graph. Please format your changes as bullet points!\n\nChange layout of regions. The graph is too tall. Maybe make a grid of regions, or combine onto one graph with region as colors depending on density.\nChange coloring of regions. There is red and green on the graph and they are right next to each other in the original graph too.\nRemove legend, the regions are labeled on the graph.\nLabel fewer (or no) countries.\ny-axis doesn’t make sense"
  },
  {
    "objectID": "posts/lab2/index.html#improving-the-bad-visualization",
    "href": "posts/lab2/index.html#improving-the-bad-visualization",
    "title": "Lab 2",
    "section": "",
    "text": "The data for the Wellcome Global Monitor 2018 report can be downloaded at the following site: https://wellcome.ac.uk/reports/wellcome-global-monitor/2018\n\nThere are two worksheets in the downloaded dataset file. You may need to read them in separately, but you may also just use one if it suffices.\n\nlibrary(tidyverse)\nlibrary(readxl)\n\ndata.summary &lt;- read_xlsx(\"wgm2018-dataset-crosstabs-all-countries.xlsx\", sheet = 1)\ndata.full &lt;- read_xlsx(\"wgm2018-dataset-crosstabs-all-countries.xlsx\", sheet = 2)\ndata.dictionary &lt;- read_xlsx(\"wgm2018-dataset-crosstabs-all-countries.xlsx\", sheet = 3)\n\ncountry.names &lt;- '1=United States, 2=Egypt, 3=Morocco, 4=Lebanon, 5=Saudi Arabia, 6=Jordan, 8=Turkey, 9=Pakistan, 10=Indonesia, 11=Bangladesh, 12=United Kingdom, 13=France, 14=Germany, 15=Netherlands, 16=Belgium, 17=Spain, 18=Italy, 19=Poland, 20=Hungary, 21=Czech Republic, 22=Romania, 23=Sweden, 24=Greece, 25=Denmark, 26=Iran, 28=Singapore, 29=Japan, 30=China, 31=India, 32=Venezuela, 33=Brazil, 34=Mexico, 35=Nigeria, 36=Kenya, 37=Tanzania, 38=Israel, 39=Palestinian Territories, 40=Ghana, 41=Uganda, 42=Benin, 43=Madagascar, 44=Malawi, 45=South Africa, 46=Canada, 47=Australia, 48=Philippines, 49=Sri Lanka, 50=Vietnam, 51=Thailand, 52=Cambodia, 53=Laos, 54=Myanmar, 55=New Zealand, 57=Botswana, 60=Ethiopia, 61=Mali, 62=Mauritania, 63=Mozambique, 64=Niger, 65=Rwanda, 66=Senegal, 67=Zambia, 68=South Korea, 69=Taiwan, 70=Afghanistan, 71=Belarus, 72=Georgia, 73=Kazakhstan, 74=Kyrgyzstan, 75=Moldova, 76=Russia, 77=Ukraine, 78=Burkina Faso, 79=Cameroon, 80=Sierra Leone, 81=Zimbabwe, 82=Costa Rica, 83=Albania, 84=Algeria, 87=Argentina, 88=Armenia, 89=Austria, 90=Azerbaijan, 96=Bolivia, 97=Bosnia and Herzegovina, 99=Bulgaria, 100=Burundi, 103=Chad, 104=Chile, 105=Colombia, 106=Comoros, 108=Republic of Congo, 109=Croatia, 111=Cyprus, 114=Dominican Republic, 115=Ecuador, 116=El Salvador, 119=Estonia, 121=Finland, 122=Gabon, 124=Guatemala, 125=Guinea, 128=Haiti, 129=Honduras, 130=Iceland, 131=Iraq, 132=Ireland, 134=Ivory Coast, 137=Kuwait, 138=Latvia, 140=Liberia, 141=Libya, 143=Lithuania, 144=Luxembourg, 145=Macedonia, 146=Malaysia, 148=Malta, 150=Mauritius, 153=Mongolia, 154=Montenegro, 155=Namibia, 157=Nepal, 158=Nicaragua, 160=Norway, 163=Panama, 164=Paraguay, 165=Peru, 166=Portugal, 173=Serbia, 175=Slovakia, 176=Slovenia, 183=Eswatini, 184=Switzerland, 185=Tajikistan, 186=The Gambia, 187=Togo, 190=Tunisia, 191=Turkmenistan, 193=United Arab Emirates, 194=Uruguay, 195=Uzbekistan, 197=Yemen, 198=Kosovo, 202=Northern Cyprus' \n\ncountry.names &lt;- strsplit(country.names, split = \",\")\ncountry.names &lt;- country.names[[1]]\ncountry.names &lt;- as_tibble(country.names)\ncountry.names &lt;- separate_wider_delim(country.names, \n                                      delim = \"=\",\n                                      cols = c(\"value\"), \n                                      names = c(\"WP5\", \"country\"))\n\ncountry.names &lt;- country.names %&gt;% mutate(WP5 = as.numeric(WP5))\ndata.full &lt;- merge(data.full, country.names, by = \"WP5\")\n\nregion.names &lt;- \"0=Not assigned, 1=Eastern Africa,2=Central Africa,3=North Africa,4=Southern Africa,5=Western Africa,6=Central America and Mexico,7=Northern America,8=South America,9=Central Asia,10=East Asia,11=Southeast Asia,12=South Asia,13=Middle East,14=Eastern Europe,15=Northern Europe,16=Southern Europe,17=Western Europe,18=Aus/NZ\"\n\nregion.names &lt;- strsplit(region.names, split = \",\")\nregion.names &lt;- region.names[[1]]\nregion.names &lt;- as_tibble(region.names)\nregion.names &lt;- separate_wider_delim(region.names, \n                                     delim = \"=\", \n                                     cols = c(\"value\"), \n                                     names = c(\"Regions_Report\", \"Region_Name\"))\n\nregion.names &lt;- region.names %&gt;% mutate(Regions_Report = as.numeric(Regions_Report))\ndata.full &lt;- merge(data.full, region.names, by = \"Regions_Report\")\n\ndata.full &lt;- data.full %&gt;% mutate(region = case_when(\n    Regions_Report %in% c(1, 2, 4, 5) ~ \"Sub-Saharan Africa\",\n    Regions_Report %in% c(6, 7, 8) ~ \"Americas\",\n    Regions_Report %in% c(9, 10, 11, 12, 18) ~ \"Asia\",\n    Regions_Report %in% c(3, 13) ~ \"Middle East and North Africa\",\n    Regions_Report %in% c(15, 16, 17, 0) ~ \"Europe\",\n    Regions_Report %in% c(14) ~ \"Former Soviet Union\"\n  ))\n\n\nImprove the visualization above by either re-creating it with the issues you identified fixed OR by creating a new visualization that you believe tells the same story better.\n\n\npercents &lt;- data.full %&gt;% select(country, region, Q25) %&gt;% \n  drop_na() %&gt;%\n  mutate(agree = ifelse(Q25 &lt; 3, 1, 0)) %&gt;% \n  group_by(country, region) %&gt;%\n  summarise(totagree = sum(agree), \n            tot = n(), \n            pctagree = totagree/tot)\n\nmedians &lt;- percents %&gt;%\n  group_by(region)%&gt;%\n  summarise(medregion = median(pctagree))\n\n\n\npercents %&gt;%\n    ggplot() +\n    geom_vline(data = medians, aes(xintercept = medregion), color = \"black\")+\n    geom_dotplot(aes(x = pctagree, fill = region), \n                 method = \"histodot\", \n                 dotsize = 0.6, \n                 binwidth = 0.01) +\n    facet_wrap(~region, ncol = 1)+\n    geom_text(aes(x = 0.3, y = 0.5, label = region, color = region), \n              hjust = \"inward\", \n              size =5, \n              family = \"Times\",\n              fontface = \"bold\")+\n    scale_fill_brewer(palette = \"Set2\") +\n    scale_color_brewer(palette = \"Set2\")+\n    theme_minimal() +\n    theme(legend.position = \"none\",\n      panel.grid.major.y = element_blank(),\n      panel.grid.minor.y = element_blank(),\n      axis.text.y = element_blank(),\n      strip.text = element_blank(),\n      text= element_text(family = \"Times\"),\n      plot.title = element_text(face = \"bold\")) +\n      scale_x_continuous(labels = scales::percent_format(accuracy = 1))+\n      labs(title = \"Percent of people who believe vaccines are safe by country and global region\", subtitle = \"Black lines represent region median\", \n           x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nI decided to create a dotplot to better show the distributions of the regions, and make better use of the y-axis. I also removed labels because they were distracting and messy, and did not include all countries."
  },
  {
    "objectID": "posts/lab2/index.html#second-data-visualization-improvement",
    "href": "posts/lab2/index.html#second-data-visualization-improvement",
    "title": "Lab 2",
    "section": "Second Data Visualization Improvement",
    "text": "Second Data Visualization Improvement\nFor this second plot, you must select a plot that uses maps so you can demonstrate your proficiency with the leaflet package!\n\nSelect a data visualization in the report that you think could be improved. Be sure to cite both the page number and figure title. Do your best to tell the story of this graph in words. That is, what is this graph telling you? What do you think the authors meant to convey with it?\nChart 5.7, page 121\n\nList the variables that appear to be displayed in this visualization.\nCountry, % of people claiming to have ever vaccinated their children (Q28)\nNow that you’re versed in the grammar of graphics (ggplot), list the aesthetics used and which variables are specified for each.\nfill = % of children vaccinated\nWhat type of graph would you call this?\nMap\nList all of the problems or things you would improve about this graph.\n\nThe less than 70% category color is very close to the country not surveyed color.\nToo much text in title/subtitle - people will not read all of that\nIn my opinion, green on a map does not necessarily convey that there is data being communicated on this graph- the color green is often associated with land in general. It could be beneficial to change the color so people don’t just scroll past this map.\n\nImprove the visualization above by either re-creating it with the issues you identified fixed OR by creating a new visualization that you believe tells the same story better.\n\n\nlibrary(leaflet)\nlibrary(forcats)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\n\nvaccinepct &lt;- data.full %&gt;% select(country, Q28) %&gt;% drop_na() %&gt;% \n  group_by(country) %&gt;% \n  summarise(tot = n(), \n            totyes = sum(Q28==1), \n            pctyes = totyes/tot) %&gt;%\n  mutate(pctgroup = case_when(\n    pctyes &lt; 0.7 ~ 1,\n    pctyes &gt;= 0.7 & pctyes &lt; 0.8 ~ 2,\n    pctyes &gt;= 0.8 & pctyes &lt; 0.9 ~ 3,\n    pctyes &gt;= 0.9 & pctyes &lt; 0.95 ~ 4,\n    pctyes &gt;= 0.95 ~ 5\n  ))\n\nmapdata &lt;-  ne_countries(scale = \"medium\", returnclass = \"sf\")\n\nvaccinepct &lt;- vaccinepct %&gt;%\n  mutate(country = case_when(\n    country == \"Eswatini\" ~ \"Kingdom of eSwatini\",\n    country == \"Ivory Coast\" ~ \"Côte d'Ivoire\",\n    country == \"Laos\" ~ \"Lao PDR\",\n    country == \"Macedonia\" ~ \"North Macedonia\",\n    country == \"Palestinian Territories\" ~ \"Palestine\",\n    country == \"Republic of Congo\" ~ \"Republic of the Congo\",\n    country == \"Russia\" ~ \"Russian Federation\",\n    country == \"South Korea\" ~ \"Republic of Korea\",\n    TRUE ~ country), \n    name_long = country)\n\nvaccinepct[which(!(vaccinepct$name_long %in% mapdata$name_long)), 1]\n\n# A tibble: 0 × 1\n# ℹ 1 variable: country &lt;chr&gt;\n\nmergedmap &lt;- left_join(mapdata, vaccinepct, by = \"name_long\")\n\nmergedmap &lt;- mergedmap %&gt;%\n  mutate(pctgroup = case_when(\n    pctyes &lt; 0.7 ~ 1,\n    pctyes &gt;= 0.7 & pctyes &lt; 0.8 ~ 2,\n    pctyes &gt;= 0.8 & pctyes &lt; 0.9 ~ 3,\n    pctyes &gt;= 0.9 & pctyes &lt; 0.95 ~ 4,\n    pctyes &gt;= 0.95 ~ 5,\n    is.na(pctyes) ~ 0),\n    percentlabel = round(pctyes * 100, 2),\n    percentlabel = ifelse(is.na(percentlabel), \"Not surveyed\", paste(percentlabel,\"%\")),\n    grouplabel = case_when(\n      pctgroup == 0 ~ \"Not surveyed\",\n      pctgroup == 1 ~ \"Less than 70%\",\n      pctgroup == 2 ~ \"70% to 79%\",\n      pctgroup == 3 ~ \"80% to 89%\",\n      pctgroup == 4 ~ \"90% to 94%\",\n      pctgroup == 5 ~ \"95% and above\"\n    ), \n    grouplabel = fct_reorder(grouplabel, pctgroup))\n\nmappal &lt;- colorNumeric('RdPu', domain = mergedmap$pctgroup)\nlabelpal &lt;- colorFactor('RdPu', domain = mergedmap$grouplabel)\n\nleaflet(mergedmap) %&gt;%\n  addTiles() %&gt;%\n  addPolygons(\n    fillColor = ~ mappal(pctgroup),\n    weight = 1, fillOpacity = 0.7,\n    color = \"lightgray\",\n    label = ~ paste(name_long, \":\", percentlabel)) %&gt;%\n  addLegend(\n    pal = labelpal, values = ~ grouplabel,  \n    title = \"People who say their children have been vaccinated\"\n  )"
  },
  {
    "objectID": "posts/lab2/index.html#third-data-visualization-improvement",
    "href": "posts/lab2/index.html#third-data-visualization-improvement",
    "title": "Lab 2",
    "section": "Third Data Visualization Improvement",
    "text": "Third Data Visualization Improvement\nFor this third plot, you must use one of the other ggplot2 extension packages mentioned this week (e.g., gganimate, plotly, patchwork, cowplot).\n\nSelect a data visualization in the report that you think could be improved. Be sure to cite both the page number and figure title. Do your best to tell the story of this graph in words. That is, what is this graph telling you? What do you think the authors meant to convey with it?\nChart 4.5, page 84\n\nThis chart is trying to categorize people based on their response to two of the questions. There seems to be the most people in the “Enthusiasts” and “The Included” category. Other than that I have really no idea what they are trying to say with this graph. I think it is poorly done.\nList the variables that appear to be displayed in this visualization.\nResponse type, views on extent to which science benefits society (country level), views on extent to which science benefits people normally (personal level)\nNow that you’re versed in the grammar of graphics (ggplot), list the aesthetics used and which variables are specified for each.\nThis seems to be a geom_point(), with aes(x= personal level, y = country level, fill = category, size = percentage in each category)\nWhat type of graph would you call this?\nBubble chart/ plot\nList all of the problems or things you would improve about this graph.\n\nI don’t think this is necessarily a good graph to tell the story that is trying to be told. I think a bar chart would be better, as people are not good at identifying/comparing area/size of circles.\nIf it is important to tell the story of where every category is located on the plot, a scatterplot could be good\nIt is not clear what each category corresponds to in terms of answers given.\nAxis labels :(\n\nImprove the visualization above by either re-creating it with the issues you identified fixed OR by creating a new visualization that you believe tells the same story better.\n\n\nlibrary(cowplot)\nlibrary(RColorBrewer)\n\nscience &lt;- data.full %&gt;% \n  select(Q17, Q18, ViewOfScience) %&gt;% \n  filter(ViewOfScience != 99, Q18 %in% c(1,2)) %&gt;%\n  mutate(ViewOfScience = as.factor(ViewOfScience),\n         label = case_when(\n           ViewOfScience == 1 ~ \"Enthusiast \\nYes, Yes\",\n           ViewOfScience == 2 ~ \"Included \\nYes, No\",\n           ViewOfScience == 3 ~ \"Excluded \\nNo, Yes\",\n           ViewOfScience == 4 ~ \"Sceptic \\nNo, No\"\n         ))\n\nscience_long &lt;- pivot_longer(science, cols = c(Q17, Q18), \n                            names_to = \"Q\", values_to = \"Response\") %&gt;%\n  mutate(Response = factor(Response),\n         label = ifelse(Response == 1, \"Yes\", \"No\"))\n\nstacked &lt;- ggplot(data = science, aes(x = \"\", fill = ViewOfScience))+\n  geom_bar(position = \"fill\")+\n  geom_text(aes(label = label), \n            stat = \"count\",\n            position = position_fill(vjust = 0.5),\n            family = \"Times\") +\n  scale_y_continuous(minor_breaks = NULL, \"\", \n                     labels = scales::percent_format(accuracy = 1))+\n  scale_fill_brewer(palette = \"Paired\")+\n  theme_cowplot(12)+\n  theme(text = element_text(\"Times\"), \n        legend.position = \"none\", \n        plot.title = element_text(face = \"bold\"))+\n  labs(title = \"Combined and individual views of peoples' opinions on benefits of science on a personal and country level\",\n       subtitle = \"Labels on left show overall and single views on 1) individual and 2) country level benefits of science\",\n       x = \"Combined Opinion\")\n\n\n\nsidebyside &lt;- ggplot(data = science_long,\n                    aes(x = Q, fill = Response))+\n  geom_bar(position = \"fill\")+\n  geom_text(aes(label = label), \n            stat = \"count\",\n            position = position_fill(vjust = 0.5),\n            family = \"Times\")+ \n  scale_y_continuous(breaks = NULL, \"\", \n                     labels = element_blank(),\n                     )+\n  scale_x_discrete(labels = c(\"Individual Level\", \"Country level\"))+\n  theme_cowplot(12)+\n  theme(text = element_text(\"Times\"), \n        legend.position = \"none\", \n        plot.title = element_text(face = \"bold\"))+\n  labs(title = \"\",\n       x = \"Single Opinion\") +\n  scale_fill_manual(values = brewer.pal(12, \"Paired\")[9:10])\n\nplot_grid(stacked, sidebyside)"
  },
  {
    "objectID": "posts/gradereflection/index.html",
    "href": "posts/gradereflection/index.html",
    "title": "Final Grade Reflection",
    "section": "",
    "text": "These objectives show your ability to seek out new information and adapt to new tools to solve data analysis problems.\n\n\n\nI can find and adopt new packages to accomplish tasks.\nI can adapt to different syntax styles (tidy, base, formula style, data.table).\n\n\n\n\n\n\nLab 2: leaflet\n\nmappal &lt;- colorNumeric('RdPu', domain = mergedmap$pctgroup)\nlabelpal &lt;- colorFactor('RdPu', domain = mergedmap$grouplabel)\n\nleaflet(mergedmap) %&gt;%\n  addTiles() %&gt;%\n  addPolygons(\n    fillColor = ~ mappal(pctgroup),\n    weight = 1, fillOpacity = 0.7,\n    color = \"lightgray\",\n    label = ~ paste(name_long, \":\", percentlabel)) %&gt;%\n  addLegend(\n    pal = labelpal, values = ~ grouplabel,  \n    title = \"People who say their children have been vaccinated\"\n  )\n\nIn lab 2, I was introduced to the leaflet package for the first time. I was able to create an interactive map filled by colors using functions from the package colorNumeric and colorFactor. I demonstrated a solid understanding of creating maps, labelling them, and creating interactive features.\nLab 4: (code seen on GitHub)\n\ncheckboxGroupInput(\"selected_regions\", \"Select Regions:\",\n                   choices = unique(percents$region),\n                   selected = unique(percents$region))\n\ncheckboxInput(\"show_median\", \"Show Region Medians\", value = TRUE)\n\n\n output$avgcard &lt;- renderText({\n  filtered_data &lt;- percents %&gt;%\n    filter(region %in% input$selected_regions)\n  \n  mean &lt;- mean(filtered_data$pctagree)\n  \n  paste(round(mean * 100, 2), \"%\")\n})\n\nIn lab 4, I was experience using Shiny for the first time. This was a completely new way of interacting with R for me, and I enjoyed being able to create apps. I feel I succeeded in learning this skill in a timely manner and implementing Shiny in a meaningful way.\nLab 6: enquos\n\nimpute_missing &lt;- function(df, ..., impute_fun = mean) {\n  selected_cols &lt;- rlang::enquos(...)\n  \n  # warning message for no columns selected\n  if (length(selected_cols) == 0) {\n    warning(\"No columns specified for imputation, returning original dataframe\")\n    return(df)\n  }\n  \n  # selecting df columns, !!! allows for unquoting (... is usable)\n  filtered_df &lt;- df %&gt;% dplyr::select(!!!selected_cols)\n  col_names &lt;- names(filtered_df)\n  \n# ... \n  # function continues in lab \n\nIn lab 6, I wrote functions with enquos (!!!) for the first time. I was able to adjust to the new functions and syntax to create working functions.\nLab 7: APIs\nIn lab 7, I learned how to call to APIs using the httr, tidyjson and jsonlite packages. These were all new to me. I was able to successfully adapt to using json data files (with the help of my teammates) and pull data from these APIs using the GET() function and check the status of the request.\n\n\n\n\n\nI can use online resources (e.g., Google, ChatGPT, StackOverflow, YouTube) to solve problems, debug, or find new tools.\nI can use tutorials, etc. to enhance my understanding of new concepts.\nI can find source code for similar projects to use as starting points for my own.\n\n\n\n\n\n\nLab 2:\n\nleaflet(mergedmap) %&gt;%\n .... %&gt;%\n  addLegend(\n    pal = labelpal, values = ~ grouplabel,  \n    title = \"People who say their children have been vaccinated\"\n  )\n\nLab 2 presented an issue with properly labeling the leaflet map. Since this was a new package for me, troubleshooting was difficult. So, I turned to ChatGPT to help fix the addLegend function, specifically the pal and values arguments. ChatGPT was successful and provided me with the lines of code pictured above.\nLab 3: (code seen on GitHub)\nLab 3 was a fun chance to play around with stylizing options, specifically themes and icons. I used online resources to help me with these steps and to determine what I wanted my dashboard to look like. Below are two resources I used:\nIcons used in quick cards Theming\nThis website: (code seen on GitHub)\n\nwebsite:\n    title: \"Lily Cook\"\n    navbar:\n     left:\n      - href: index.qmd\n        text: Home\n      - href: about.qmd\n        text: About\n      - href: BlogPosts.qmd \n        text: Blog\n     right:\n      - icon: github\n        href:  https://github.com/lilypcook/lab-3\n        aria-label: GitHub\n      - icon: linkedin\n        href: https://www.linkedin.com/in/lilypaintoncook/\n        aria-label: LinkedIn\n    favicon: \"media/flower-icon.jpg\"\n\nWhen working on this website, I found that there was no icon on the browser tab when, and I did not like that, so I turned to the internet to help, and it was successful. This post on Stack Overflow pointed me in the right direction."
  },
  {
    "objectID": "posts/gradereflection/index.html#il-1-adding-new-skills",
    "href": "posts/gradereflection/index.html#il-1-adding-new-skills",
    "title": "Final Grade Reflection",
    "section": "",
    "text": "I can find and adopt new packages to accomplish tasks.\nI can adapt to different syntax styles (tidy, base, formula style, data.table).\n\n\n\n\n\n\nLab 2: leaflet\n\nmappal &lt;- colorNumeric('RdPu', domain = mergedmap$pctgroup)\nlabelpal &lt;- colorFactor('RdPu', domain = mergedmap$grouplabel)\n\nleaflet(mergedmap) %&gt;%\n  addTiles() %&gt;%\n  addPolygons(\n    fillColor = ~ mappal(pctgroup),\n    weight = 1, fillOpacity = 0.7,\n    color = \"lightgray\",\n    label = ~ paste(name_long, \":\", percentlabel)) %&gt;%\n  addLegend(\n    pal = labelpal, values = ~ grouplabel,  \n    title = \"People who say their children have been vaccinated\"\n  )\n\nIn lab 2, I was introduced to the leaflet package for the first time. I was able to create an interactive map filled by colors using functions from the package colorNumeric and colorFactor. I demonstrated a solid understanding of creating maps, labelling them, and creating interactive features.\nLab 4: (code seen on GitHub)\n\ncheckboxGroupInput(\"selected_regions\", \"Select Regions:\",\n                   choices = unique(percents$region),\n                   selected = unique(percents$region))\n\ncheckboxInput(\"show_median\", \"Show Region Medians\", value = TRUE)\n\n\n output$avgcard &lt;- renderText({\n  filtered_data &lt;- percents %&gt;%\n    filter(region %in% input$selected_regions)\n  \n  mean &lt;- mean(filtered_data$pctagree)\n  \n  paste(round(mean * 100, 2), \"%\")\n})\n\nIn lab 4, I was experience using Shiny for the first time. This was a completely new way of interacting with R for me, and I enjoyed being able to create apps. I feel I succeeded in learning this skill in a timely manner and implementing Shiny in a meaningful way.\nLab 6: enquos\n\nimpute_missing &lt;- function(df, ..., impute_fun = mean) {\n  selected_cols &lt;- rlang::enquos(...)\n  \n  # warning message for no columns selected\n  if (length(selected_cols) == 0) {\n    warning(\"No columns specified for imputation, returning original dataframe\")\n    return(df)\n  }\n  \n  # selecting df columns, !!! allows for unquoting (... is usable)\n  filtered_df &lt;- df %&gt;% dplyr::select(!!!selected_cols)\n  col_names &lt;- names(filtered_df)\n  \n# ... \n  # function continues in lab \n\nIn lab 6, I wrote functions with enquos (!!!) for the first time. I was able to adjust to the new functions and syntax to create working functions.\nLab 7: APIs\nIn lab 7, I learned how to call to APIs using the httr, tidyjson and jsonlite packages. These were all new to me. I was able to successfully adapt to using json data files (with the help of my teammates) and pull data from these APIs using the GET() function and check the status of the request."
  },
  {
    "objectID": "posts/gradereflection/index.html#il-2-online-resources",
    "href": "posts/gradereflection/index.html#il-2-online-resources",
    "title": "Final Grade Reflection",
    "section": "",
    "text": "I can use online resources (e.g., Google, ChatGPT, StackOverflow, YouTube) to solve problems, debug, or find new tools.\nI can use tutorials, etc. to enhance my understanding of new concepts.\nI can find source code for similar projects to use as starting points for my own.\n\n\n\n\n\n\nLab 2:\n\nleaflet(mergedmap) %&gt;%\n .... %&gt;%\n  addLegend(\n    pal = labelpal, values = ~ grouplabel,  \n    title = \"People who say their children have been vaccinated\"\n  )\n\nLab 2 presented an issue with properly labeling the leaflet map. Since this was a new package for me, troubleshooting was difficult. So, I turned to ChatGPT to help fix the addLegend function, specifically the pal and values arguments. ChatGPT was successful and provided me with the lines of code pictured above.\nLab 3: (code seen on GitHub)\nLab 3 was a fun chance to play around with stylizing options, specifically themes and icons. I used online resources to help me with these steps and to determine what I wanted my dashboard to look like. Below are two resources I used:\nIcons used in quick cards Theming\nThis website: (code seen on GitHub)\n\nwebsite:\n    title: \"Lily Cook\"\n    navbar:\n     left:\n      - href: index.qmd\n        text: Home\n      - href: about.qmd\n        text: About\n      - href: BlogPosts.qmd \n        text: Blog\n     right:\n      - icon: github\n        href:  https://github.com/lilypcook/lab-3\n        aria-label: GitHub\n      - icon: linkedin\n        href: https://www.linkedin.com/in/lilypaintoncook/\n        aria-label: LinkedIn\n    favicon: \"media/flower-icon.jpg\"\n\nWhen working on this website, I found that there was no icon on the browser tab when, and I did not like that, so I turned to the internet to help, and it was successful. This post on Stack Overflow pointed me in the right direction."
  },
  {
    "objectID": "posts/gradereflection/index.html#rw-1-file-code-and-data-management",
    "href": "posts/gradereflection/index.html#rw-1-file-code-and-data-management",
    "title": "Final Grade Reflection",
    "section": "[RW-1] File, code, and data management:",
    "text": "[RW-1] File, code, and data management:\n\nI can use Git and GitHub to track my progress (creating repos, cloning, forking, pull requesting).\nI always use R Projects and the {here} package to organize my scripts, notebooks, data, and applications.\nI always use pull requests when collaborating with others.\n\n\nLevel: 3\n\n\nJustification:\nThis is surprisingly the first class I was required to use Git and GitHub for. I have done a good job learning and implementing Github, especially creating repositories. My repositories are here. I am able to commit and push in both RStudio and the terminal. Using GitHub, I was able to use pull requests and adjust workflow for issues that arose with pull requests in a fork rather than the original repo. Our group figured out how to merge our edits using the terminal in order to successfully collaborate on the same file. I think the biggest justification of my GitHub skills is that this website is created and hosted with GitHub. I have been able to implement GitHub for other projects, including my thesis. Coming from 0 GitHub experience to having a somewhat complex website hosted with is a testament to me meeting this learning goal."
  },
  {
    "objectID": "posts/gradereflection/index.html#rw-2-notebooks",
    "href": "posts/gradereflection/index.html#rw-2-notebooks",
    "title": "Final Grade Reflection",
    "section": "[RW-2] Notebooks:",
    "text": "[RW-2] Notebooks:\n\nI can use Quarto to produce a reproducible notebook and polished rendered documents\nI can use appropriate chunk options (echo, error, cache, etc.) to render my Quarto document quickly and cleanly.\n\n\nLevel: 3\n\n\nJustification:\nLab 1: Stylizing document\n\nMake this text bold.\nMake this text italicized.\nMake these into a bullet point list:\n\n\nApples\nBananas\nPotatoes\n\nIn Lab 1, I demonstrated the ability to stylize Quarto documents, including text as seen above. Additionally, in this Lab I demonstrated my ability to utilize code chunk options, including #| eval: false, #| echo: false and #| layout-nrow: 1\nLab 3: (code seen on GitHub): Chunk options for creating dashboard\n\n---\ntitle: \"Lab 3 Dashboard - Lily Cook\"\nformat: \n  dashboard: \n    theme: minty\n    orientation: columns\n    nav-buttons: \n     - icon: github\n       href:  https://github.com/lilypcook/lab-3\n       aria-label: GitHub\n     - icon: linkedin\n       href: https://www.linkedin.com/in/lilypaintoncook/\n       aria-label: LinkedIn\nlogo: worldlogo.png\n---\n\nIn Lab 3, I was able to use YAML options, seen above, to turn my document into a Quarto dashboard. In this document, I was able to use various HTML commands to set formatting including {.tabset} and {.width = 80%} Additionally, I set these code chunks so they would not render in this document. Overall, I have shown the ability to appropriately format multiple documents using Quarto and HTML languages."
  },
  {
    "objectID": "posts/gradereflection/index.html#rw-3-code-style",
    "href": "posts/gradereflection/index.html#rw-3-code-style",
    "title": "Final Grade Reflection",
    "section": "[RW-3] Code style",
    "text": "[RW-3] Code style\n\nMy code is clear, readable, well-organized, and well-commented.\n\n\nLevel: 3\n\n\nJustification\nLab 3: (code seen on GitHub)\n\ndata.full &lt;- data.full %&gt;% \n  mutate(region = case_when(\n    Regions_Report %in% c(1, 2, 4, 5) ~ \"Sub-Saharan Africa\",\n    Regions_Report %in% c(6, 7, 8) ~ \"Americas\",\n    Regions_Report %in% c(9, 10, 11, 12, 18) ~ \"Asia\",\n    Regions_Report %in% c(3, 13) ~ \"Middle East and North Africa\",\n    Regions_Report %in% c(15, 16, 17, 0) ~ \"Europe\",\n    Regions_Report %in% c(14) ~ \"Former Soviet Union\"\n  ))\n\n\nvaccinepct &lt;- data.full %&gt;% \n  select(country, Q28) %&gt;% \n  drop_na() %&gt;% \n  group_by(country) %&gt;% \n  summarise(tot = n(), \n            totyes = sum(Q28 == 1), \n            pctyes = totyes/tot) %&gt;%\n  mutate(pctgroup = case_when(\n    pctyes &lt; 0.7 ~ 1,\n    pctyes &gt;= 0.7 & pctyes &lt; 0.8 ~ 2,\n    pctyes &gt;= 0.8 & pctyes &lt; 0.9 ~ 3,\n    pctyes &gt;= 0.9 & pctyes &lt; 0.95 ~ 4,\n    pctyes &gt;= 0.95 ~ 5\n  ))\n\nThis snippet of code from Lab 3 demonstrated my coding style. I use clean style, with the package dplyr to clean the data. I name the variables I create logical and informative names, to ensure that they will be easy to find when needed. I also make sure to name all chunks in labs. The use of %in% shows an improvement in style from the beginning of the quarter.\nLab 9: example of improved commenting\n\n# generating random variables using various distributions\nw &lt;- rpois(300, 70)\nx &lt;- rgamma(300, 3)\ny &lt;- rnorm(300, 50, 200)\nz &lt;- rbinom(300, 200, 0.7)\na &lt;- sample(c('a', 'b', 'c'), replace = T, size = 300)\n\ndata &lt;- cbind(w, x, y, z, a) # combining into df\n\n\n# create gradient color palette\npal &lt;- colorRampPalette(c(\"lightgoldenrod1\",\"azure3\", \"slateblue\"))\n\nart_colors &lt;- pal(n_distinct(w)) # select amount of colors needed for art\n\n#plot\n\nggplot(data, aes(x = y, y = z)) +\n  geom_point(aes(shape = a, color = w), show.legend = F, alpha = 0.9) + #add points\n  geom_line(aes(group = w, color = w), linetype = 3, show.legend = F, alpha = 0.3)+ # connect points\n  scale_shape_manual(values = c(\"a\" = 20, \"b\" = 8, \"c\" = 21)) + # change shapes of points\n  scale_color_manual(values = art_colors) + # change colors\n  coord_radial() + #circle\n  theme_void() + #remove everything\n  theme(panel.background = element_rect(fill = \"midnightblue\", color = \"midnightblue\")) # background color\n\nAt the midterm evaluation I said “Where I can improve to reach virtuoso status is adding more comments to the code for added reproducibility.” The above example shows improved commenting in my lab work. This lab was individual, but what really helped me improve my commenting was working in teams for previous labs. Based on this improvement, I believe I have earned a virtuoso in this learning objective."
  },
  {
    "objectID": "posts/gradereflection/index.html#rw-summary",
    "href": "posts/gradereflection/index.html#rw-summary",
    "title": "Final Grade Reflection",
    "section": "RW Summary",
    "text": "RW Summary"
  },
  {
    "objectID": "posts/gradereflection/index.html#tc-1-project-summaries",
    "href": "posts/gradereflection/index.html#tc-1-project-summaries",
    "title": "Final Grade Reflection",
    "section": "[TC-1] Project summaries:",
    "text": "[TC-1] Project summaries:\n\nI can create clear and succinct summaries of a project.\nI accurately interpret statistical or modeling results.\nI consider the appropriate scope and impact of my project results.\n\n\nLevel: 3\n\n\nJustification:\nLab 4: (code seen on GitHub)\n\n#| content: valuebox\n#| title: Average Country-Wide Belief in Safety of Vaccines\n#| color: info\n#| icon: globe\n\nlist(value = paste(round(avgpercent * 100,2), \"%\"))\n\nIn Labs 3 and 4, I created succinct visualizations and value boxes that provided quick and clear summaries of the Wellcome analysis. Above is an example chunk of a value box I created in lab 4, but looking at the dashboards themselves provide better evidence. The dashboards provide a good overview of the analysis done in digestible chunks for non-statistically minded people. I sent this dashboard to my mom (who is a immunization nurse) and she was able to understand all visualizations.\nWe have mostly only done the first point of this learning objective in class, so I don’t have evidence for the second bullet points."
  },
  {
    "objectID": "posts/gradereflection/index.html#tc-2-documentation",
    "href": "posts/gradereflection/index.html#tc-2-documentation",
    "title": "Final Grade Reflection",
    "section": "[TC-2] Documentation:",
    "text": "[TC-2] Documentation:\n\nI can create a user-friendly dashboard.\nI provide ample documentation for my custom functions.\n\n\nLevel: 3\n\n\nJustification:\nLab 3: (code seen on GitHub): Labeling of dashboard documents and overall layout\n\n---\ntitle: \"Lab 3 Dashboard - Lily Cook\"\nformat: \n  dashboard: \n    theme: minty\n    orientation: columns\n    nav-buttons: \n     - icon: github\n       href:  https://github.com/lilypcook/lab-3\n       aria-label: GitHub\n     - icon: linkedin\n       href: https://www.linkedin.com/in/lilypaintoncook/\n       aria-label: LinkedIn\nlogo: worldlogo.png\n---\n\nIn Lab 3, I created my first dashboard from scratch. The above chunk shows the YAML for the layout used. This layout ensured a straightforward user experience. Additionally, I utilized tabset and page options, to avoid too much information on one page. I also appropriate organization and labels for these plots to ensure a good user experience on the dashboard.\nLab 4: (code seen on GitHub): Clear user inputs\n\ncheckboxGroupInput(\"selected_regions\", \"Select Regions:\",\n                   choices = unique(percents$region),\n                   selected = unique(percents$region))\n\ncheckboxInput(\"show_median\", \"Show Region Medians\", value = TRUE)\n\nLab 4 had a similar dashboard to Lab 3. I had similar successes in Lab 4 as with Lab 3. To extend this in Lab 4, I used user-friendly user inputs, so it was clear what was needed from the user to interact with the dashboard. Again, I sent this dashboard to my parents, and it was clear for them to interact with, they had no questions."
  },
  {
    "objectID": "posts/gradereflection/index.html#tc-summary",
    "href": "posts/gradereflection/index.html#tc-summary",
    "title": "Final Grade Reflection",
    "section": "TC Summary",
    "text": "TC Summary"
  },
  {
    "objectID": "posts/gradereflection/index.html#dm-1-data-preparation",
    "href": "posts/gradereflection/index.html#dm-1-data-preparation",
    "title": "Final Grade Reflection",
    "section": "[DM-1] Data Preparation",
    "text": "[DM-1] Data Preparation\n\nI can read in datasets to R, including untidy ones.\nI can clean datasets to deal with missing data, typos, poor formatting, etc.\n\n\nLevel: 2\n\n\nJustification:\nLab 2:\n\nlibrary(tidyverse)\nlibrary(readxl)\n\ndata.summary &lt;- read_xlsx(\"wgm2018-dataset-crosstabs-all-countries.xlsx\", sheet = 1)\ndata.full &lt;- read_xlsx(\"wgm2018-dataset-crosstabs-all-countries.xlsx\", sheet = 2)\ndata.dictionary &lt;- read_xlsx(\"wgm2018-dataset-crosstabs-all-countries.xlsx\", sheet = 3)\n\ncountry.names &lt;- '1=United States, 2=Egypt, 3=Morocco, 4=Lebanon, 5=Saudi Arabia, 6=Jordan, 8=Turkey, 9=Pakistan, 10=Indonesia, 11=Bangladesh, 12=United Kingdom, 13=France, 14=Germany, 15=Netherlands, 16=Belgium, 17=Spain, 18=Italy, 19=Poland, 20=Hungary, 21=Czech Republic, 22=Romania, 23=Sweden, 24=Greece, 25=Denmark, 26=Iran, 28=Singapore, 29=Japan, 30=China, 31=India, 32=Venezuela, 33=Brazil, 34=Mexico, 35=Nigeria, 36=Kenya, 37=Tanzania, 38=Israel, 39=Palestinian Territories, 40=Ghana, 41=Uganda, 42=Benin, 43=Madagascar, 44=Malawi, 45=South Africa, 46=Canada, 47=Australia, 48=Philippines, 49=Sri Lanka, 50=Vietnam, 51=Thailand, 52=Cambodia, 53=Laos, 54=Myanmar, 55=New Zealand, 57=Botswana, 60=Ethiopia, 61=Mali, 62=Mauritania, 63=Mozambique, 64=Niger, 65=Rwanda, 66=Senegal, 67=Zambia, 68=South Korea, 69=Taiwan, 70=Afghanistan, 71=Belarus, 72=Georgia, 73=Kazakhstan, 74=Kyrgyzstan, 75=Moldova, 76=Russia, 77=Ukraine, 78=Burkina Faso, 79=Cameroon, 80=Sierra Leone, 81=Zimbabwe, 82=Costa Rica, 83=Albania, 84=Algeria, 87=Argentina, 88=Armenia, 89=Austria, 90=Azerbaijan, 96=Bolivia, 97=Bosnia and Herzegovina, 99=Bulgaria, 100=Burundi, 103=Chad, 104=Chile, 105=Colombia, 106=Comoros, 108=Republic of Congo, 109=Croatia, 111=Cyprus, 114=Dominican Republic, 115=Ecuador, 116=El Salvador, 119=Estonia, 121=Finland, 122=Gabon, 124=Guatemala, 125=Guinea, 128=Haiti, 129=Honduras, 130=Iceland, 131=Iraq, 132=Ireland, 134=Ivory Coast, 137=Kuwait, 138=Latvia, 140=Liberia, 141=Libya, 143=Lithuania, 144=Luxembourg, 145=Macedonia, 146=Malaysia, 148=Malta, 150=Mauritius, 153=Mongolia, 154=Montenegro, 155=Namibia, 157=Nepal, 158=Nicaragua, 160=Norway, 163=Panama, 164=Paraguay, 165=Peru, 166=Portugal, 173=Serbia, 175=Slovakia, 176=Slovenia, 183=Eswatini, 184=Switzerland, 185=Tajikistan, 186=The Gambia, 187=Togo, 190=Tunisia, 191=Turkmenistan, 193=United Arab Emirates, 194=Uruguay, 195=Uzbekistan, 197=Yemen, 198=Kosovo, 202=Northern Cyprus' \n\ncountry.names &lt;- strsplit(country.names, split = \",\")\ncountry.names &lt;- country.names[[1]]\ncountry.names &lt;- as_tibble(country.names)\ncountry.names &lt;- separate_wider_delim(country.names, \n                                      delim = \"=\",\n                                      cols = c(\"value\"), \n                                      names = c(\"WP5\", \"country\"))\n\ncountry.names &lt;- country.names %&gt;% mutate(WP5 = as.numeric(WP5))\ndata.full &lt;- merge(data.full, country.names, by = \"WP5\")\n\nregion.names &lt;- \"0=Not assigned, 1=Eastern Africa,2=Central Africa,3=North Africa,4=Southern Africa,5=Western Africa,6=Central America and Mexico,7=Northern America,8=South America,9=Central Asia,10=East Asia,11=Southeast Asia,12=South Asia,13=Middle East,14=Eastern Europe,15=Northern Europe,16=Southern Europe,17=Western Europe,18=Aus/NZ\"\n\nregion.names &lt;- strsplit(region.names, split = \",\")\nregion.names &lt;- region.names[[1]]\nregion.names &lt;- as_tibble(region.names)\nregion.names &lt;- separate_wider_delim(region.names, \n                                     delim = \"=\", \n                                     cols = c(\"value\"), \n                                     names = c(\"Regions_Report\", \"Region_Name\"))\n\nregion.names &lt;- region.names %&gt;% mutate(Regions_Report = as.numeric(Regions_Report))\n\ndata.full &lt;- merge(data.full, region.names, by = \"Regions_Report\")\n\ndata.full &lt;- data.full %&gt;% \n  mutate(region = case_when(\n    Regions_Report %in% c(1, 2, 4, 5) ~ \"Sub-Saharan Africa\",\n    Regions_Report %in% c(6, 7, 8) ~ \"Americas\",\n    Regions_Report %in% c(9, 10, 11, 12, 18) ~ \"Asia\",\n    Regions_Report %in% c(3, 13) ~ \"Middle East and North Africa\",\n    Regions_Report %in% c(15, 16, 17, 0) ~ \"Europe\",\n    Regions_Report %in% c(14) ~ \"Former Soviet Union\"\n  ))\n\nLab 2 had a very messy data to read in. It was an Excel file with three different sheets, with less than ideal data. This demonstrated my ability to use multiple types of data, not just csv files. I was able to utilize dplyr and other packages to successfully clean data. Working with string data is complicated, and I was able use multiple tools like strsplit() and separate_wider_delim() to work with them. However, I do have room for improvement. The long strings pulled out were entries in the data dictionary and I used the merge() function, rather than the cleaner join() options.\nLab 4: (code seen on GitHub)\n\ndata.full &lt;- read_csv(\"datafull.csv\")\n\nIn Lab 4, the same dataset was used. I utilized data pre-processing and saved the data with write.csv() so I did not need to process this data everytime this app was rendered, which was necessary for publishing on Shiny. Above is how I was able to read in the csv using readr package."
  },
  {
    "objectID": "posts/gradereflection/index.html#dm-2-data-wrangling",
    "href": "posts/gradereflection/index.html#dm-2-data-wrangling",
    "title": "Final Grade Reflection",
    "section": "[DM-2] Data Wrangling",
    "text": "[DM-2] Data Wrangling\n\nI can cleverly use pivoting, separating, grouping, and joining to wrangle data.\nI can use mapping (purrr) to perform repeated tasks.\n\n\nLevel: 3\n\n\nJustification:\nLab 3: (code seen on GitHub)\n\nmapdata &lt;-  ne_countries(scale = \"medium\", returnclass = \"sf\")\n\nvaccinepct &lt;- vaccinepct %&gt;%\n  mutate(country = case_when(\n    country == \"Eswatini\" ~ \"Kingdom of eSwatini\",\n    country == \"Ivory Coast\" ~ \"Côte d'Ivoire\",\n    country == \"Laos\" ~ \"Lao PDR\",\n    country == \"Macedonia\" ~ \"North Macedonia\",\n    country == \"Palestinian Territories\" ~ \"Palestine\",\n    country == \"Republic of Congo\" ~ \"Republic of the Congo\",\n    country == \"Russia\" ~ \"Russian Federation\",\n    country == \"South Korea\" ~ \"Republic of Korea\",\n    TRUE ~ country), \n    name_long = country)\n\n#vaccinepct[which(!(vaccinepct$name_long %in% mapdata$name_long)), 1]\n\nmergedmap &lt;- left_join(mapdata, vaccinepct, by = \"name_long\")\n\nmergedmap &lt;- mergedmap %&gt;%\n  mutate(pctgroup = case_when(\n    pctyes &lt; 0.7 ~ 1,\n    pctyes &gt;= 0.7 & pctyes &lt; 0.8 ~ 2,\n    pctyes &gt;= 0.8 & pctyes &lt; 0.9 ~ 3,\n    pctyes &gt;= 0.9 & pctyes &lt; 0.95 ~ 4,\n    pctyes &gt;= 0.95 ~ 5,\n    is.na(pctyes) ~ 0),\n    percentlabel = round(pctyes * 100, 2),\n    percentlabel = ifelse(is.na(percentlabel), \"Not surveyed\", paste(percentlabel,\"%\")),\n    grouplabel = case_when(\n      pctgroup == 0 ~ \"Not surveyed\",\n      pctgroup == 1 ~ \"Less than 70%\",\n      pctgroup == 2 ~ \"70% to 79%\",\n      pctgroup == 3 ~ \"80% to 89%\",\n      pctgroup == 4 ~ \"90% to 94%\",\n      pctgroup == 5 ~ \"95% and above\"\n    ), \n    grouplabel = fct_reorder(grouplabel, pctgroup))\n\nIn labs 2 and 3, to create a leaflet map, I had to use map data that came from a JSON file from the rnaturalearth package. This demonstates my ability to wrangle data, as I was able to indentify differences in two files using which() and merge the files (this time using more appropriate left_join). This code chunk also demonstrates my ability to create new variables for analysis using case_when and ifelse(). Again, I demonstrate my familiarity with dplyr functions to group, separate, and reorder data. I justify my virtuoso status here with my visible improvements from the code chunk in the previous learning objective."
  },
  {
    "objectID": "posts/gradereflection/index.html#dm-3-data-formats",
    "href": "posts/gradereflection/index.html#dm-3-data-formats",
    "title": "Final Grade Reflection",
    "section": "[DM-3] Data Formats",
    "text": "[DM-3] Data Formats\n\nI can use API urls to access JSON data and convert it into a data frame\nI can scrape data from the web and convert it into a data frame\n\n\n## Fill in the line below with your self-assigned level for this objective.\nmy_level &lt;- 3\n\nportfolio_levels &lt;- portfolio_levels %&gt;%\n  bind_rows(tibble(Objective = \"DM-3\", \n                   Level = my_level))\n\n\nLevel: 3\n\n\nJustification\nLab 7: Use of APIs\n\n# function to get pass times for a single capital\n\nget_single_location_passes &lt;- function(lat, long) {\n  api_base_url &lt;- \"https://api.g7vrd.co.uk/v1/satellite-passes/25544\"\n  request_url &lt;- paste0(api_base_url, \"/\", lat, \"/\", long, \".json\")\n  \n  # get request\n  response &lt;- GET(request_url)\n  \n  # check if the request successful, if success get content\n  if (status_code(response) == 200) {\n    content &lt;- content(response, \"text\", encoding = \"UTF-8\")\n    pass_data &lt;- fromJSON(content)\n    \n    # extract time \n    if (length(pass_data$passes) &gt; 0 && \"tca\" %in% names(pass_data$passes)) {\n      # get first 3 TCA timestamps\n      return(head(pass_data$passes$tca, 3))\n    } else {\n      return(NULL) # if no passes found or tca field missing\n    }\n  } else {\n    warning(paste(\"API request failed for lat:\", lat, \"long:\", long, \"with status:\", status_code(response)))\n    return(NULL) # if request failed\n  }\n}\n\n# function to get pass times for capitals and create df\nget_all_capitals_passes &lt;- function(df) {\n  # empty list to store results\n  all_passes_list &lt;- list()\n  \n  # loop through each capital\n  for (i in 1:nrow(df)) {\n    capital_info &lt;- df[i, ]\n    \n    pass_times_vector &lt;- get_single_location_passes(lat = capital_info$lat, long = capital_info$long)\n    \n    # make tibble for the capital pass times\n    # three time columns, NA if less than 3 \n    current_passes_df &lt;- tibble(\n      state_abbr = capital_info$state,\n      capital_name = capital_info$capital,\n      lat = capital_info$lat,\n      long = capital_info$long,\n      time1 = if (length(pass_times_vector) &gt;= 1) pass_times_vector[1] else NA_character_,\n      time2 = if (length(pass_times_vector) &gt;= 2) pass_times_vector[2] else NA_character_,\n      time3 = if (length(pass_times_vector) &gt;= 3) pass_times_vector[3] else NA_character_\n    )\n    \n    all_passes_list[[i]] &lt;- current_passes_df\n    \n    # wait for 1 second between requests\n    Sys.sleep(1) \n  }\n  \n  # combine all tibbles\n  final_df &lt;- bind_rows(all_passes_list)\n  return(final_df)\n}\n\nIn Lab 8, our team collaborated to pull data from APIs. Though it was a bit of a learning process, we successfully retrieved data from the APIs. This including writing a function to get data from multiple URLs. Overall, we were successful in this task and our code was reproducible."
  },
  {
    "objectID": "posts/gradereflection/index.html#dm-summary",
    "href": "posts/gradereflection/index.html#dm-summary",
    "title": "Final Grade Reflection",
    "section": "DM Summary",
    "text": "DM Summary"
  },
  {
    "objectID": "posts/gradereflection/index.html#pv-1-clear-accessible-visualizations",
    "href": "posts/gradereflection/index.html#pv-1-clear-accessible-visualizations",
    "title": "Final Grade Reflection",
    "section": "[PV-1] Clear & Accessible Visualizations",
    "text": "[PV-1] Clear & Accessible Visualizations\n\nI can make my plots more clear by removing the legend and adding annotations.\nI can edit the titles, subtitles, captions, axis labels, etc. to create a clearly labelled plot.\nI can choose colors (“scales”) and themes to make a visually pleasing and accessible plot.\n\n\nLevel: 3\n\n\nJustification:\nLab 2: improvements of visualizations\n\npercents &lt;- data.full %&gt;% select(country, region, Q25) %&gt;% \n  drop_na() %&gt;%\n  mutate(agree = ifelse(Q25 &lt; 3, 1, 0)) %&gt;% \n  group_by(country, region) %&gt;%\n  summarise(totagree = sum(agree), \n            tot = n(), \n            pctagree = totagree/tot)\n\nmedians &lt;- percents %&gt;%\n  group_by(region)%&gt;%\n  summarise(medregion = median(pctagree))\n\n\n\npercents %&gt;%\n    ggplot() +\n    geom_vline(data = medians, aes(xintercept = medregion), color = \"black\")+\n    geom_dotplot(aes(x = pctagree, fill = region), \n                 method = \"histodot\", \n                 dotsize = 0.6, \n                 binwidth = 0.01) +\n    facet_wrap(~region, ncol = 1)+\n    geom_text(aes(x = 0.3, y = 0.5, label = region, color = region), \n              hjust = \"inward\", \n              size =5, \n              family = \"Times\",\n              fontface = \"bold\")+\n    scale_fill_brewer(palette = \"Set2\") +\n    scale_color_brewer(palette = \"Set2\")+\n    theme_minimal() +\n    theme(legend.position = \"none\",\n      panel.grid.major.y = element_blank(),\n      panel.grid.minor.y = element_blank(),\n      axis.text.y = element_blank(),\n      strip.text = element_blank(),\n      text= element_text(family = \"Times\"),\n      plot.title = element_text(face = \"bold\")) +\n      scale_x_continuous(labels = scales::percent_format(accuracy = 1))+\n      labs(title = \"Percent of people who believe vaccines are safe by country and global region\", subtitle = \"Black lines represent region median\", \n           x = \"\", y = \"\")\n\nLab 2 focused on improving visualizations. Above is the code for the dotplot. I removed the legend and messy labels, insted replacing these with test over the relevant facets. This uses a color-blind friendly brewer palette (Set2), and text and data points are color coded. I have a clear title and an informative sub-title. I have minimal gridlines and a single x-axis to avoid unnecessary distraction. Overall, I follow the grammar of graphic guidelines outline in the Ted Talk we watched at the beginning of the quarter."
  },
  {
    "objectID": "posts/gradereflection/index.html#pv-2-dynamic-visualizations",
    "href": "posts/gradereflection/index.html#pv-2-dynamic-visualizations",
    "title": "Final Grade Reflection",
    "section": "[PV-2] Dynamic Visualizations",
    "text": "[PV-2] Dynamic Visualizations\n\nI can use a package like {gganimate} to create self-contained gifs.\nI can use a package like {plotly}, {ggplotly}, {leaflet}, {ggirafe}, etc. to make interactive html widgets.\n\n\nLevel: 2\n\n\nJustification:\nLab 2: leaflet\n\nmappal &lt;- colorNumeric('RdPu', domain = mergedmap$pctgroup)\nlabelpal &lt;- colorFactor('RdPu', domain = mergedmap$grouplabel)\n\nleaflet(mergedmap) %&gt;%\n  addTiles() %&gt;%\n  addPolygons(\n    fillColor = ~ mappal(pctgroup),\n    weight = 1, fillOpacity = 0.7,\n    color = \"lightgray\",\n    label = ~ paste(name_long, \":\", percentlabel)) %&gt;%\n  addLegend(\n    pal = labelpal, values = ~ grouplabel,  \n    title = \"People who say their children have been vaccinated\"\n  )\n\nIn Lab 2, I created an interactive leaflet, including labels that appear when hovering over a country. This map was clear and well labeled, and a good representation of my dynamic plot skills.\nLab 7: leaflet\n\n# ensure there is data for plot\nif (nrow(iss_pass_data_processed) &gt; 0) {\n  \n  # format times (local time zone for readability) for labels etc\n  \n  map_data &lt;- iss_pass_data_processed |&gt;\n    mutate(\n      time1_display = format(time1_dt, \"%Y-%m-%d %H:%M:%S UTC\"),\n      time2_display = format(time2_dt, \"%Y-%m-%d %H:%M:%S UTC\"),\n      time3_display = format(time3_dt, \"%Y-%m-%d %H:%M:%S UTC\"),\n      \n      # handle NA times in display strings\n      time2_display = ifelse(is.na(time2_dt), \"N/A\", time2_display),\n      time3_display = ifelse(is.na(time3_dt), \"N/A\", time3_display)\n    )\n\n  # hover labels\n  hover_labels &lt;- paste0(\n    \"&lt;strong&gt;Capital:&lt;/strong&gt; \", map_data$capital_name, \"&lt;br&gt;\",\n    \"&lt;strong&gt;State:&lt;/strong&gt; \", map_data$state_abbr, \"&lt;br&gt;\",\n    \"&lt;strong&gt;Soonest Pass:&lt;/strong&gt; \", map_data$time1_display\n  ) |&gt; lapply(htmltools::HTML) # lapply with HTML for proper rendering\n\n  # click popups\n  click_popups &lt;- paste0(\n    \"&lt;strong&gt;Capital:&lt;/strong&gt; \", map_data$capital_name, \" (\", map_data$state_abbr, \")&lt;br&gt;&lt;br&gt;\",\n    \"&lt;strong&gt;Predicted Pass Times (UTC):&lt;/strong&gt;&lt;br&gt;\",\n    \"1. \", map_data$time1_display, \"&lt;br&gt;\",\n    \"2. \", map_data$time2_display, \"&lt;br&gt;\",\n    \"3. \", map_data$time3_display\n  ) |&gt; lapply(htmltools::HTML)\n\n  # create the map\n  iss_map &lt;- leaflet(data = map_data) |&gt;\n    addTiles(group = \"OSM (Default)\") |&gt; # add default OpenStreetMap map tiles\n    addProviderTiles(providers$CartoDB.Positron, group = \"CartoDB Positron\") |&gt;\n    addProviderTiles(providers$Esri.WorldImagery, group = \"Esri World Imagery\") |&gt;\n    \n    # markers for each state capital\n    addMarkers(\n      lng = ~long, \n      lat = ~lat,\n      icon = satellite_icon,\n      label = hover_labels,\n      popup = click_popups,\n      group = \"State Capitals\"\n    ) |&gt;\n    \n    #  polylines connecting capitals in order of first pass time\n    addPolylines(\n      lng = ~long,\n      lat = ~lat,\n      color = \"#E6007E\", \n      weight = 3,\n      opacity = 0.8,\n      dashArray = \"5, 5\", # dashed line\n      group = \"ISS Pass Order Path\"\n    ) |&gt;\n    \n    #  layer controls to toggle layers\n    addLayersControl(\n      baseGroups = c(\"OSM (Default)\", \"CartoDB Positron\", \"Esri World Imagery\"),\n      overlayGroups = c(\"State Capitals\", \"ISS Pass Order Path\"),\n      options = layersControlOptions(collapsed = FALSE)\n    ) |&gt;\n    \n    # legend for the polyline\n    addLegend(\n        position = \"bottomright\",\n        colors = \"#E6007E\",\n        labels = \"ISS Pass Order Path\",\n        title = \"Map Features\"\n    )\n\n  # display the map\n  iss_map\n\n} else {\n  print(\"No ISS pass data available to map. Check API calls or data processing steps.\")\n}\n\nIn lab 7, we extended our leaflet abilities and made a much more in-depth leaflet, including using a cute rocket as an icon for the markers. This shows mastery of leaflet. Despite this, I still feel I earned a proficient here, as I did not extend my knowledge to as many packages as I would have liked to."
  },
  {
    "objectID": "posts/gradereflection/index.html#pv-3-interactive-visualizations",
    "href": "posts/gradereflection/index.html#pv-3-interactive-visualizations",
    "title": "Final Grade Reflection",
    "section": "[PV-3] Interactive Visualizations",
    "text": "[PV-3] Interactive Visualizations\n\nI can use Shiny or webR to create visualizations that react to a user’s input.\n\n\nLevel: 3\n\n\nJustification:\nLab 4: (code seen on GitHub) Shiny Interactivity\n\nsliderInput(\"percent_filter\", \n             label = \"Minimum Vaccination Rate (%)\", \n             min = 0, \n             max = 100, \n             value = 50, \n             step = 1)\n\n\ncheckboxGroupInput(\"selected_regions2\", \"Select Regions:\",\n                   choices = unique(percents$region),\n                   selected = unique(percents$region))\n\nIn Lab 4, I utilized Shiny to create an interactive visualization, value box and table. Above is the code used to create user inputs for the table, including a slider and check boxes. The interactive table responded to two types of user input, demonstrating my ability in interactive visualizations. The interactive dot plot in this dashboard also responded to multiple inputs. Though this table could use improvement in organization, the interactivity of it is well done."
  },
  {
    "objectID": "posts/gradereflection/index.html#pv-summary",
    "href": "posts/gradereflection/index.html#pv-summary",
    "title": "Final Grade Reflection",
    "section": "PV Summary",
    "text": "PV Summary"
  },
  {
    "objectID": "posts/gradereflection/index.html#cdai-1-r-programming-language",
    "href": "posts/gradereflection/index.html#cdai-1-r-programming-language",
    "title": "Final Grade Reflection",
    "section": "[CDAI-1] R Programming Language",
    "text": "[CDAI-1] R Programming Language\n\nI understand non-standard evaluation (aka “tidy eval” or “unquoted objects”), and I can use tunneling in my functions.\nI understand functional programming, and I can use functions as objects in my code design.\n\n\nLevel: 1\n\n\nJustification:\nLab 6: function writing\nHonestly, I was not great at writing functions. I definitely could have used R in a better way. I used for loops where I could have used other functions like map. I was ok at non-standard evaluation with enquos() and handling ... but I still could have improved this. It is ok that I was not great at this, I don’t forsee myself writing any crazy functions in the future."
  },
  {
    "objectID": "posts/gradereflection/index.html#cdai-2-object-handling",
    "href": "posts/gradereflection/index.html#cdai-2-object-handling",
    "title": "Final Grade Reflection",
    "section": "[CDAI-2] Object Handling",
    "text": "[CDAI-2] Object Handling\n\nI have built in checks for possible input problems\nI can make reasonable choices in my code design about when to save intermediate objects.\nI can convert objects between types and structures as needed.\n\n\nLevel: 2\n\n\nJustification:\nLab 6:\n\nimpute_missing &lt;- function(df, ..., impute_fun = mean) {\n  selected_cols &lt;- rlang::enquos(...)\n  \n  # warning message for no columns selected\n  if (length(selected_cols) == 0) {\n    warning(\"No columns specified for imputation, returning original dataframe\")\n    return(df)\n  }\n  \n  # selecting df columns, !!! allows for unquoting (... is usable)\n  filtered_df &lt;- df %&gt;% dplyr::select(!!!selected_cols)\n  col_names &lt;- names(filtered_df)\n  \n  # warning message for when all columns do not exist\n  if (length(col_names) == 0) {\n     stop(\"Columns specified do not exist\")\n  }\n  \n  # checking is all cols numeric \n  are_numeric &lt;- sapply(filtered_df, is.numeric)\n  if (!all(are_numeric)) {\n    # id non-numeric and warning\n    non_numeric_selected_cols &lt;- col_names[!are_numeric]\n    stop(paste(\"The following columns are non-numeric:\", paste(non_numeric_selected_cols, collapse=\", \")))\n  }\n  \n  df_imputed &lt;- df %&gt;%\n    mutate(\n      across(\n        .cols = all_of(col_names), #selecting cols\n        .fns = ~ {imputation_val &lt;- impute_fun(.x, na.rm = TRUE) #creating impute values\n                  tidyr::replace_na(.x, imputation_val) #replacing NAs\n                  }\n      ))\n  \n  return(df_imputed)\n}\n\nThough the actual function itself could have been improved, in the impute_missing() function I did a good job writing error messages for this function that were informative for the user. Where I could have improved here is saving intermediate objects. I feel I saved too many, which could slow down the running of the function."
  },
  {
    "objectID": "posts/gradereflection/index.html#cdai-3-speed-and-efficiency",
    "href": "posts/gradereflection/index.html#cdai-3-speed-and-efficiency",
    "title": "Final Grade Reflection",
    "section": "[CDAI-3] Speed and Efficiency",
    "text": "[CDAI-3] Speed and Efficiency\n\nI can recognize moments of possible slowdown in my code, and use built-in functions or parallelizing to speed them up.\nI always use and design vectorized functions whenever possible.\n\n\nLevel: 2\n\n\nJustification:\nLab 8: helper functions\n\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(stringr)\n\n# defining url and pages to look at\nbase_url &lt;- \"https://www.cheese.com/alphabetical/?per_page=100\"\npage_numbers &lt;- 1:21\n\n# Helper function to extract text or attributes based on tag structure\nextract_info &lt;- function(page, outer_selector, inner_selector, attr = NULL) {\n  nodes &lt;- page %&gt;%\n    html_elements(outer_selector) %&gt;%\n    html_elements(inner_selector)\n  \n  if (!is.null(attr)) { #dealing with null values\n    html_attr(nodes, attr)\n  } else {\n    html_text(nodes)\n  }\n}\n\n# Function to scrape a single page\nscrape_cheese_page &lt;- function(page_number) {\n  full_url &lt;- paste0(base_url, \"&page=\", page_number)\n  page &lt;- read_html(full_url)\n\n  data.frame(\n    Name = extract_info(page, \"div.product-item\", \"h3\"), #cheese name\n    url = paste0(\"https://www.cheese.com\", extract_info(page, \"div.product-item\", \"h3 a\", \"href\")), #cheese url\n    whether = extract_info(page, \"div.product-item\", \"img\", \"class\"), #if there is image\n    stringsAsFactors = FALSE\n  )\n}\n\n# Map over all pages and bind results\ncheese_data &lt;- map_dfr(page_numbers, function(pg) {\n  result &lt;- scrape_cheese_page(pg)\n  Sys.sleep(1)  # delay to be nice\n  result\n})\n\nhead(cheese_data)\n\nIn lab 8, the helper functions we wrote to help with web scraping were vectorized. I gave myself a proficient here since I may have difficulties identifying slow downs in a function if it came down to it."
  },
  {
    "objectID": "posts/gradereflection/index.html#cdai-4-supporting-functions",
    "href": "posts/gradereflection/index.html#cdai-4-supporting-functions",
    "title": "Final Grade Reflection",
    "section": "[CDAI-4] Supporting Functions",
    "text": "[CDAI-4] Supporting Functions\n\nI write helper / shortcut functions to streamline repeated tasks and make my code easier to read.\nI use intermediate functions to streamline repeated or looping processes.\n\n\nLevel: 3\n\n\nJustification:\nLab 8: same as above\nLab 7\n\n# function to get pass times for a single capital\n\nget_single_location_passes &lt;- function(lat, long) {\n  api_base_url &lt;- \"https://api.g7vrd.co.uk/v1/satellite-passes/25544\"\n  request_url &lt;- paste0(api_base_url, \"/\", lat, \"/\", long, \".json\")\n  \n  # get request\n  response &lt;- GET(request_url)\n  \n  # check if the request successful, if success get content\n  if (status_code(response) == 200) {\n    content &lt;- content(response, \"text\", encoding = \"UTF-8\")\n    pass_data &lt;- fromJSON(content)\n    \n    # extract time \n    if (length(pass_data$passes) &gt; 0 && \"tca\" %in% names(pass_data$passes)) {\n      # get first 3 TCA timestamps\n      return(head(pass_data$passes$tca, 3))\n    } else {\n      return(NULL) # if no passes found or tca field missing\n    }\n  } else {\n    warning(paste(\"API request failed for lat:\", lat, \"long:\", long, \"with status:\", status_code(response)))\n    return(NULL) # if request failed\n  }\n}\n\n# function to get pass times for capitals and create df\nget_all_capitals_passes &lt;- function(df) {\n  # empty list to store results\n  all_passes_list &lt;- list()\n  \n  # loop through each capital\n  for (i in 1:nrow(df)) {\n    capital_info &lt;- df[i, ]\n    \n    pass_times_vector &lt;- get_single_location_passes(lat = capital_info$lat, long = capital_info$long)\n    \n    # make tibble for the capital pass times\n    # three time columns, NA if less than 3 \n    current_passes_df &lt;- tibble(\n      state_abbr = capital_info$state,\n      capital_name = capital_info$capital,\n      lat = capital_info$lat,\n      long = capital_info$long,\n      time1 = if (length(pass_times_vector) &gt;= 1) pass_times_vector[1] else NA_character_,\n      time2 = if (length(pass_times_vector) &gt;= 2) pass_times_vector[2] else NA_character_,\n      time3 = if (length(pass_times_vector) &gt;= 3) pass_times_vector[3] else NA_character_\n    )\n    \n    all_passes_list[[i]] &lt;- current_passes_df\n    \n    # wait for 1 second between requests\n    Sys.sleep(1) \n  }\n  \n  # combine all tibbles\n  final_df &lt;- bind_rows(all_passes_list)\n  return(final_df)\n}\n\nReferring to the code above, the helper functions we wrote were incredibly useful in our web scraping. I must acknowledge how useful it was to complete this lab in group settings and the teamwork that went into these functions. We were able to recognize where helper functions would be useful and successfully avoid repetitive code."
  },
  {
    "objectID": "posts/gradereflection/index.html#cdai-5-algorithmic-process",
    "href": "posts/gradereflection/index.html#cdai-5-algorithmic-process",
    "title": "Final Grade Reflection",
    "section": "[CDAI-5] Algorithmic Process",
    "text": "[CDAI-5] Algorithmic Process\n\nI can invent and implement my own iterative algorithm.\nMy loops are clean and efficient.\nI have built in checks for possible problems or extreme cases in the algorithm.\n\n\nLevel: 1\n\n\nJustification:\nI was able to apply map() functions in a couple of places, but largely had difficulties with loops and iterations. I relied heavily on my teammates for help on the iterations. I have had previous classes where for loops were the standard (I was not taught how to use map until this class) and had a hard time breaking away from them. Overall, I could largely improve here and my lack of data science courses had a negative impact on my skills. I will also not likely use these skills in the future, so I am okay with this."
  },
  {
    "objectID": "posts/gradereflection/index.html#cdai-6-generative-art",
    "href": "posts/gradereflection/index.html#cdai-6-generative-art",
    "title": "Final Grade Reflection",
    "section": "[CDAI-6] Generative Art",
    "text": "[CDAI-6] Generative Art\n\nI can apply a variety of generative art functions to make a visually pleasing piece.\nI can explain why particular changes to the code result in particular differences in the visualization.\n\n\nLevel: 3\n\n\nJustification:\nLab 9\n(code not included here since it was very long)\nThis lab was awesome and the most fun assignment I have done in a while. In this lab, I demonstrated that I can create art using both ggplot2 workflow/ functions as well as more generative options. Though I used Danielle’s functions for a piece, I knew what was happening and could justify my choices. I decided to choose my own colors for more fun."
  },
  {
    "objectID": "posts/gradereflection/index.html#cdai-summary",
    "href": "posts/gradereflection/index.html#cdai-summary",
    "title": "Final Grade Reflection",
    "section": "CDAI Summary",
    "text": "CDAI Summary"
  },
  {
    "objectID": "posts/gradereflection/index.html#grade",
    "href": "posts/gradereflection/index.html#grade",
    "title": "Final Grade Reflection",
    "section": "Grade",
    "text": "Grade\nBased on the summary plot above, I believe I have earned a B+ (maybeeee an A-) in STAT 541.\n\nJustification\nI feel that I am largely proficient the learning objectives presented in this course, other than a couple involving function writing. I certainly improved in my general code style (especially commenting code) since the midterm and met many of the goals I set for myself. I have been able to use many of the skills used outside of this course, especially in my thesis. I feel that I have learned a lot in this course and can explain all code. I have displayed much improvement in coding style and ability since the beginning of this course. More importantly, I have found many of these assignments to be fun, which is a huge attitude shift towards R for me. I especially enjoyed the generative art assignment and had fun exploring all of the ggplot options I never knew existed. I did have trouble individually writing functions (Lab 6) but when working in a team, it really clicked for me, especially since I had help from my group mates who had completed the data science minor and more experienced in these things.\nBelow is a summary of my self-scored learning outcomes that we have completed in class so far:\n[RW-1] File, Code, and Data Management: Virtuoso\n[RW-2] Notebooks: Virtuoso\n[RW-3] Code Style: Virtuoso\n[DM-1] Data Preparation: Proficient\n[DM-2] Data Wrangling: Virtuoso\n[DM-3] Data Formats: Virtuoso\n[PV-1] Clear & Accessible Visualizations: Virtuoso\n[PV-2] Dynamic Visualizations: Proficient\n[PV-3] Interactive Visualizations: Virtuoso\n[IL-1] Adding New Skills: Virtuoso\n[IL-2] Online Resources: Virtuoso\n[TC-1] Project Summaries: Virtuoso\n[TC-2] Documentation: Virtuoso\n[CDAI-1] R Programming Language: Novice\n[CDAI-2] Object Handling: Proficient\n[CDAI-3] Speed and Efficiency: Proficient\n[CDAI-4] Supporting Functions: Virtuoso\n[CDAI-5] Algorithmic Process: Novice\n[CDAI-6] Generative Art: Virtuoso\nFor all virtuoso and proficient scores, I feel I have well justified my scores. For my two novice scores, I have outlined my shortcomings.\nI was hoping to end the course with an A, but I think my function writing abilities are holding me back here. I feel I have shown some improvement throughout the quarter and had much more fun than expected! Working in groups has assisted me in understanding where I can improve with my code writing. I like to think that I have shown enough improvement in my work and overall done a good enough job to earn an A-, but I would be satisfied with a B+ in this course, if you feel that better reflects my skills."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lily Cook",
    "section": "",
    "text": "Hi, I’m Lily. I am a graduate student at Cal Poly, San Luis Obispo currently wrapping up my MS in Statistics, on track to graduate June 2025. I am excited to enter the work force as a data analyst. My time at Cal Poly has equipped me with a robust understanding of statistical methodologies, data modeling, and the power of extracting meaningful insights from complex datasets. I’m eager to apply these skills to help organizations make data-driven decisions and solve real-world problems. I’m a curious and analytical thinker with a passion for uncovering patterns and communicating findings clearly and effectively. I’m excited to connect and explore opportunities in the data analysis field!"
  },
  {
    "objectID": "posts/lab9/index.html",
    "href": "posts/lab9/index.html",
    "title": "Lab 9",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tibble)\nlibrary(ggforce)\nlibrary(deldir)\nlibrary(ggthemes)\nlibrary(tictoc)\nlibrary(ambient)\nlibrary(purrr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(sf)"
  },
  {
    "objectID": "posts/lab9/index.html#midnight-stargazing",
    "href": "posts/lab9/index.html#midnight-stargazing",
    "title": "Lab 9",
    "section": "Midnight Stargazing",
    "text": "Midnight Stargazing\n\n# generating random variables using various distributions\nw &lt;- rpois(300, 70)\nx &lt;- rgamma(300, 3)\ny &lt;- rnorm(300, 50, 200)\nz &lt;- rbinom(300, 200, 0.7)\na &lt;- sample(c('a', 'b', 'c'), replace = T, size = 300)\n\ndata &lt;- cbind(w, x, y, z, a) # combining into df\n\n\n# create gradient color pallete\npal &lt;- colorRampPalette(c(\"lightgoldenrod1\",\"azure3\", \"slateblue\"))\n\nart_colors &lt;- pal(n_distinct(w))\n\n#plot\n\nggplot(data, aes(x = y, y = z)) +\n  geom_point(aes(shape = a, color = w), show.legend = F, alpha = 0.9) + #add points\n  geom_line(aes(group = w, color = w), linetype = 3, show.legend = F, alpha = 0.3)+ # connect points\n  scale_shape_manual(values = c(\"a\" = 20, \"b\" = 8, \"c\" = 21)) + # change shapes of points\n  scale_color_manual(values = art_colors) + # change colors\n  coord_radial() + #circle\n  theme_void() + #remove everything\n  theme(panel.background = element_rect(fill = \"midnightblue\", color = \"midnightblue\")) # background color\n\n\n\n\n\n\n\n\n\nDescription\nGazing at the midnight stars, your eyes drift between the celestial bodies. The stars seem to be randomly scattered across the sky…..\n\n\nCode choices\nBy adding coord_radial(), the art piece now has a circular shape. Setting the background fill and color gives the same midnight blue color as the backdrop of the piece. Adjusting the alpha in the geoms adjusts the opacity of the shapes on the plot. scale_color_manual allows me to choose a palette and scale_shape_manual allows me to choose the shapes the points will be. Lastly, using theme_void removes all titles, axis lines, etc. to give the appearance of art."
  },
  {
    "objectID": "posts/lab9/index.html#central-coast-spring",
    "href": "posts/lab9/index.html#central-coast-spring",
    "title": "Lab 9",
    "section": "Central Coast Spring",
    "text": "Central Coast Spring\n\n# defining palette that I want\nmy_pal &lt;- c(\"#4A5d23\", \"#708238\", \"#568203\",\"#8A9A5B\", \"#6B8E23\", \"#A0522D\", \"#8B4513\", \"#7e481c\", \"goldenrod1\")\n\n\n\n# function to randomly select row from danielle\nchoose_rectangle &lt;- function(blocks) {\n  sample(nrow(blocks), 1, prob = blocks$area)\n}\n\n# function to randomly set rectangle breaks from danielle\nchoose_break &lt;- function(lower, upper) {\n  round((upper - lower) * runif(1))\n}\n\n# function to create tibble of rectangles from danielle\ncreate_rectangles &lt;- function(left, right, bottom, top, value) {\n  tibble(\n    left = left,\n    right = right,\n    bottom = bottom,\n    top = top,\n    width = right - left,\n    height = top - bottom,\n    area = width * height,\n    value = value\n  )\n}\n\n# danielle function to split rectangle horizontally\nsplit_rectangle_x &lt;- function(rectangle, new_value) {\n  with(rectangle, {\n    split &lt;- choose_break(left, right)\n    new_left  &lt;- c(left, left + split)\n    new_right &lt;- c(left + split, right)\n    new_value &lt;- c(value, new_value)\n    create_rectangles(new_left, new_right, bottom, top, new_value)\n  })\n}\n\n# danielle function to split rectangle vertically\nsplit_rectangle_y &lt;- function(rectangle, new_value) {\n  with(rectangle, {\n    split &lt;- choose_break(bottom, top)\n    new_bottom &lt;- c(bottom, bottom + split)\n    new_top &lt;- c(bottom + split, top)\n    new_value &lt;- c(value, new_value)\n    create_rectangles(left, right, new_bottom, new_top, new_value)\n  })\n}\n\n# danielle function combining the two split functions\n\nsplit_rectangle &lt;- function(rectangle, value) {\n  if(runif(1) &lt; .5) {\n    return(split_rectangle_x(rectangle, value))\n  }\n  split_rectangle_y(rectangle, value)\n}\n\n# danielle function to randomly split rectangle\nsplit_block &lt;- function(blocks, value) {\n  old &lt;- choose_rectangle(blocks) \n  new &lt;- split_rectangle(blocks[old, ], value)\n  bind_rows(blocks[-old, ], new)\n}\n\n\n# danielle function to repeatedly divide rectangles\n\nsubdivision &lt;- function(ncol = 1000, \n                        nrow = 1000, \n                        nsplits = 50, \n                        seed = NULL) {\n  \n  if(!is.null(seed)) set.seed(seed)\n  blocks &lt;- create_rectangles(\n    left = 1, \n    right = ncol, \n    bottom = 1, \n    top = nrow, \n    value = 0\n  )\n  reduce(1:nsplits, split_block, .init = blocks)\n}\n\n# danielle function to fracture/ fill square basicall the start of the mosaic\nfill_rectangle &lt;- function(left, right, bottom, top, width, \n                           height, area, value, nshades = 100) {\n  \n  set.seed(value)\n  fractals &lt;- list(billow, fbm, ridged)\n  generators &lt;- list(gen_simplex, gen_perlin, gen_worley)\n  \n  expand_grid(\n    x = left:right, \n    y = bottom:top, \n  ) |&gt;\n    mutate(\n      fill = 10 * value + fracture(\n        x = x * sample(-3:3, 1),\n        y = y * sample(-3:3, 1),\n        noise = sample(generators, 1)[[1]],\n        fractal = sample(fractals, 1)[[1]],\n        octaves = sample(10, 1),\n        frequency = sample(10, 1) / 20,\n        value = \"distance2\"\n      ) |&gt;\n        normalise(to = c(1, nshades)) |&gt; \n        round()\n    )\n}\n\n# danielle function to plot squares\ndraw_mosaic &lt;- function(dat, palette) {\n  background &lt;- sample(palette[1:5], 1) #edited\n  dat |&gt;\n    ggplot(aes(x, y, fill = fill)) +\n    geom_tile(show.legend = FALSE, colour = background, size = .2) +\n    scale_size_identity() +\n    scale_colour_gradientn(colours = palette) +\n    scale_fill_gradientn(colours = palette) +\n    scale_x_continuous(expand = expansion(add = 5)) +\n    scale_y_continuous(expand = expansion(add = 5)) +\n    coord_equal() +\n    theme_void() +\n    theme(plot.background = element_rect(fill = background)) \n}\n\n\n# danielle function bringing it all together - edited palette\nmosaica &lt;- function(ncol = 60, \n                    nrow = 60, \n                    nsplits = 30, \n                    seed = NULL) {\n  \n  subdivision(ncol, nrow, nsplits, seed) |&gt;\n    pmap_dfr(fill_rectangle) |&gt; \n    slice_sample(prop = .995) |&gt;\n    filter(!is.na(fill)) |&gt;\n    draw_mosaic(palette = my_pal) #added my palette here for more control\n}\n\n\n\nmosaica(ncol = 200, nrow = 150, nsplits = 3000, seed = 541)\n\n\n\n\n\n\n\n\n\nDescription\nSpring has arrived in the central coast of California. Green grass spreads across the hills dotted with yellow blooms.\n\n\nCode Choices\nI decided to choose my own palette instead of allowing for a random palette as the shapes reminded me of fields. This was also more fun for me. I also edited the background call in the draw_mosaic function to select a green from the palette rather than the browns or yellow. In the function call to mosaica, ncol is set to 200, meaning the art will be 200 “pixels” wide, and nrow is set to 150, meaning the art will be 150 “pixels” tall. The nsplits argument is set to 3000, meaning the art will be divided into 300 “rectangles”, though the nature of the mosaic allows for the shapes to be non-rectangular (or at least not perfectly rectangular). This is a large number of splits and allows for more “detail” in the drawing"
  },
  {
    "objectID": "posts/lab6/index.html",
    "href": "posts/lab6/index.html",
    "title": "Lab 6 - Spicy",
    "section": "",
    "text": "This assignment will challenge your function writing abilities. I’m not going to lie, these functions are difficult but well within your reach. I do, however, want to recognize that not everyone is interested in being a “virtuoso” with their function writing. So, there are two options for this week’s lab:"
  },
  {
    "objectID": "posts/lab6/index.html#testing-your-function",
    "href": "posts/lab6/index.html#testing-your-function",
    "title": "Lab 6 - Spicy",
    "section": "Testing Your Function!",
    "text": "Testing Your Function!\n\n## Testing how your function handles multiple input variables\nremove_outliers(diamonds, \n                price, \n                x, \n                y, \n                z)\n\n# A tibble: 53,936 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 2  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 3  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 4  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 5  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n 6  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n 7  0.3  Good      J     SI1      64      55   339  4.25  4.28  2.73\n 8  0.23 Ideal     J     VS1      62.8    56   340  3.93  3.9   2.46\n 9  0.22 Premium   F     SI1      60.4    61   342  3.88  3.84  2.33\n10  0.31 Ideal     J     SI2      62.2    54   344  4.35  4.37  2.71\n# ℹ 53,926 more rows\n\n## Testing how your function handles an input that isn't numeric\nremove_outliers(diamonds, \n                price, \n                color)\n\n# A tibble: 53,939 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 2  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 3  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 4  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 5  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 6  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 7  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 8  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n 9  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n10  0.3  Good      J     SI1      64      55   339  4.25  4.28  2.73\n# ℹ 53,929 more rows\n\n## Testing how your function handles a non-default sd_thresh\nremove_outliers(diamonds, \n                price,\n                x, \n                y, \n                z, \n                sd_thresh = 2)\n\n# A tibble: 53,936 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 2  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 3  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 4  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 5  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n 6  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n 7  0.3  Good      J     SI1      64      55   339  4.25  4.28  2.73\n 8  0.23 Ideal     J     VS1      62.8    56   340  3.93  3.9   2.46\n 9  0.22 Premium   F     SI1      60.4    61   342  3.88  3.84  2.33\n10  0.31 Ideal     J     SI2      62.2    54   344  4.35  4.37  2.71\n# ℹ 53,926 more rows\n\n## Demonstrating error message for not-found column \nremove_outliers(diamonds, \n                lol, \n                x, \n                y, \n                z)\n\n# A tibble: 53,937 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 2  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 3  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 4  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 5  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 6  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n 7  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n 8  0.3  Good      J     SI1      64      55   339  4.25  4.28  2.73\n 9  0.23 Ideal     J     VS1      62.8    56   340  3.93  3.9   2.46\n10  0.22 Premium   F     SI1      60.4    61   342  3.88  3.84  2.33\n# ℹ 53,927 more rows\n\n\nExercise 2: Write a function that imputes missing values for numeric variables in a dataset. The user should be able to supply the dataset, the variables to impute values for, and a function to use when imputing. Hint 1: You will need to use across() to apply your function, since the user can input multiple variables. Hint 2: The replace_na() function is helpful here!\n\nimpute_missing &lt;- function(df, ..., impute_fun = mean) {\n  selected_cols &lt;- rlang::enquos(...)\n  \n  # warning message for no columns selected\n  if (length(selected_cols) == 0) {\n    warning(\"No columns specified for imputation, returning original dataframe\")\n    return(df)\n  }\n  \n  # selecting df columns, !!! allows for unquoting (... is usable)\n  filtered_df &lt;- df %&gt;% dplyr::select(!!!selected_cols)\n  col_names &lt;- names(filtered_df)\n  \n  # warning message for when all columns do not exist\n  if (length(col_names) == 0) {\n     stop(\"Columns specified do not exist\")\n  }\n  \n  # checking is all cols numeric \n  are_numeric &lt;- sapply(filtered_df, is.numeric)\n  if (!all(are_numeric)) {\n    # id non-numeric and warning\n    non_numeric_selected_cols &lt;- col_names[!are_numeric]\n    stop(paste(\"The following columns are non-numeric:\", paste(non_numeric_selected_cols, collapse=\", \")))\n  }\n  \n  df_imputed &lt;- df %&gt;%\n    mutate(\n      across(\n        .cols = all_of(col_names), #selecting cols\n        .fns = ~ {imputation_val &lt;- impute_fun(.x, na.rm = TRUE) #creating impute values\n                  tidyr::replace_na(.x, imputation_val) #replacing NAs\n                  }\n      ))\n  \n  return(df_imputed)\n}"
  },
  {
    "objectID": "posts/lab6/index.html#testing-your-function-1",
    "href": "posts/lab6/index.html#testing-your-function-1",
    "title": "Lab 6 - Spicy",
    "section": "Testing Your Function!",
    "text": "Testing Your Function!\n\n## Testing how your function handles multiple input variables\nimpute_missing(nycflights13::flights, \n               arr_delay, \n               dep_delay) \n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;\n\n## Testing how your function handles an input that isn't numeric\nimpute_missing(nycflights13::flights, \n               arr_delay, \n               carrier)\n\nError in impute_missing(nycflights13::flights, arr_delay, carrier): The following columns are non-numeric: carrier\n\n## Testing how your function handles a non-default impute_fun\nimpute_missing(nycflights13::flights, \n               arr_delay, \n               dep_delay, \n               impute_fun = median)\n\n# A tibble: 336,776 × 19\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;"
  },
  {
    "objectID": "posts/lab6/index.html#testing-your-function-2",
    "href": "posts/lab6/index.html#testing-your-function-2",
    "title": "Lab 6 - Spicy",
    "section": "Testing Your Function!",
    "text": "Testing Your Function!\n\nfit_model(\n  diamonds,\n  mod_formula = price ~ carat + cut,\n  remove_outliers = TRUE,\n  impute_missing = TRUE,\n  price, \n  carat\n)\n\n\nCall:\nlm(formula = mod_formula, data = df_filtered)\n\nCoefficients:\n(Intercept)        carat        cut.L        cut.Q        cut.C        cut^4  \n   -2701.47      7871.17      1239.78      -528.59       367.96        74.63"
  },
  {
    "objectID": "posts/lab6/index.html#parameters",
    "href": "posts/lab6/index.html#parameters",
    "title": "Lab 6 - Spicy",
    "section": "Parameters",
    "text": "Parameters\nFirst, we need to define the set of parameters we want to iterate the fit_model() function over. The tidyr package has a useful function called crossing() that is useful for generating argument combinations. For each argument, we specify all possible values for that argument and crossing() generates all combinations. Note that you can create a list of formula objects in R with c(y ~ x1, y ~ x1 + x2).\n\ndf_arg_combos &lt;- crossing(\n    impute = c(TRUE, FALSE),\n    remove_outliers = c(TRUE, FALSE), \n    mod = c(y ~ x1, \n            y ~ x1 + x2)\n)\ndf_arg_combos\n\nExercise 4: Use crossing() to create the data frame of argument combinations for our analyses.\n\ndf_arg_combos &lt;- crossing(\n    impute = c(TRUE, FALSE),\n    remove_outliers = c(TRUE, FALSE), \n    mod = c(price ~ carat, \n            price ~ carat + cut,\n            price ~ carat + cut + clarity,\n            price ~ carat + cut + clarity + color)\n)\ndf_arg_combos\n\n# A tibble: 16 × 3\n   impute remove_outliers mod      \n   &lt;lgl&gt;  &lt;lgl&gt;           &lt;list&gt;   \n 1 FALSE  FALSE           &lt;formula&gt;\n 2 FALSE  FALSE           &lt;formula&gt;\n 3 FALSE  FALSE           &lt;formula&gt;\n 4 FALSE  FALSE           &lt;formula&gt;\n 5 FALSE  TRUE            &lt;formula&gt;\n 6 FALSE  TRUE            &lt;formula&gt;\n 7 FALSE  TRUE            &lt;formula&gt;\n 8 FALSE  TRUE            &lt;formula&gt;\n 9 TRUE   FALSE           &lt;formula&gt;\n10 TRUE   FALSE           &lt;formula&gt;\n11 TRUE   FALSE           &lt;formula&gt;\n12 TRUE   FALSE           &lt;formula&gt;\n13 TRUE   TRUE            &lt;formula&gt;\n14 TRUE   TRUE            &lt;formula&gt;\n15 TRUE   TRUE            &lt;formula&gt;\n16 TRUE   TRUE            &lt;formula&gt;"
  },
  {
    "objectID": "posts/lab6/index.html#iterating-over-the-parameters",
    "href": "posts/lab6/index.html#iterating-over-the-parameters",
    "title": "Lab 6 - Spicy",
    "section": "Iterating Over the Parameters",
    "text": "Iterating Over the Parameters\nWe’ve arrived at the final step!\nExercise 5: Use pmap() from purrr to apply the fit_model() function to every combination of arguments from `diamonds.\n\npmap(df_arg_combos, fit_model, df = diamonds, price, carat)\n\n[[1]]\n\nCall:\nlm(formula = mod_formula, data = df_filtered)\n\nCoefficients:\n(Intercept)        carat  \n      -2256         7756  \n\n\n[[2]]\n\nCall:\nlm(formula = mod_formula, data = df_filtered)\n\nCoefficients:\n(Intercept)        carat        cut.L        cut.Q        cut.C        cut^4  \n   -2701.38      7871.08      1239.80      -528.60       367.91        74.59  \n\n\n[[3]]\n\nCall:\nlm(formula = mod_formula, data = df_filtered)\n\nCoefficients:\n(Intercept)        carat        cut.L        cut.Q        cut.C        cut^4  \n  -3187.540     8472.026      713.804     -334.503      188.482        1.663  \n  clarity.L    clarity.Q    clarity.C    clarity^4    clarity^5    clarity^6  \n   4011.681    -1821.922      917.658     -430.047      257.141       26.909  \n  clarity^7  \n    186.742  \n\n\n[[4]]\n\nCall:\nlm(formula = mod_formula, data = df_filtered)\n\nCoefficients:\n(Intercept)        carat        cut.L        cut.Q        cut.C        cut^4  \n  -3710.603     8886.129      698.907     -327.686      180.565       -1.207  \n  clarity.L    clarity.Q    clarity.C    clarity^4    clarity^5    clarity^6  \n   4217.535    -1832.406      923.273     -361.995      216.616        2.105  \n  clarity^7      color.L      color.Q      color.C      color^4      color^5  \n    110.340    -1910.288     -627.954     -171.960       21.678      -85.943  \n    color^6  \n    -49.986  \n\n\n[[5]]\n\nCall:\nlm(formula = mod_formula, data = df_filtered)\n\nCoefficients:\n(Intercept)        carat  \n      -2256         7757  \n\n\n[[6]]\n\nCall:\nlm(formula = mod_formula, data = df_filtered)\n\nCoefficients:\n(Intercept)        carat        cut.L        cut.Q        cut.C        cut^4  \n   -2701.47      7871.17      1239.78      -528.59       367.96        74.63  \n\n\n[[7]]\n\nCall:\nlm(formula = mod_formula, data = df_filtered)\n\nCoefficients:\n(Intercept)        carat        cut.L        cut.Q        cut.C        cut^4  \n  -3187.784     8472.316      713.686     -334.533      188.544        1.705  \n  clarity.L    clarity.Q    clarity.C    clarity^4    clarity^5    clarity^6  \n   4012.037    -1821.966      917.545     -429.913      257.052       26.916  \n  clarity^7  \n    186.767  \n\n\n[[8]]\n\nCall:\nlm(formula = mod_formula, data = df_filtered)\n\nCoefficients:\n(Intercept)        carat        cut.L        cut.Q        cut.C        cut^4  \n  -3710.814     8886.379      698.795     -327.726      180.624       -1.168  \n  clarity.L    clarity.Q    clarity.C    clarity^4    clarity^5    clarity^6  \n   4217.848    -1832.466      923.155     -361.872      216.522        2.114  \n  clarity^7      color.L      color.Q      color.C      color^4      color^5  \n    110.368    -1910.260     -627.961     -172.133       21.894      -86.104  \n    color^6  \n    -49.899  \n\n\n[[9]]\n\nCall:\nlm(formula = mod_formula, data = df_filtered)\n\nCoefficients:\n(Intercept)        carat  \n      -2256         7756  \n\n\n[[10]]\n\nCall:\nlm(formula = mod_formula, data = df_filtered)\n\nCoefficients:\n(Intercept)        carat        cut.L        cut.Q        cut.C        cut^4  \n   -2701.38      7871.08      1239.80      -528.60       367.91        74.59  \n\n\n[[11]]\n\nCall:\nlm(formula = mod_formula, data = df_filtered)\n\nCoefficients:\n(Intercept)        carat        cut.L        cut.Q        cut.C        cut^4  \n  -3187.540     8472.026      713.804     -334.503      188.482        1.663  \n  clarity.L    clarity.Q    clarity.C    clarity^4    clarity^5    clarity^6  \n   4011.681    -1821.922      917.658     -430.047      257.141       26.909  \n  clarity^7  \n    186.742  \n\n\n[[12]]\n\nCall:\nlm(formula = mod_formula, data = df_filtered)\n\nCoefficients:\n(Intercept)        carat        cut.L        cut.Q        cut.C        cut^4  \n  -3710.603     8886.129      698.907     -327.686      180.565       -1.207  \n  clarity.L    clarity.Q    clarity.C    clarity^4    clarity^5    clarity^6  \n   4217.535    -1832.406      923.273     -361.995      216.616        2.105  \n  clarity^7      color.L      color.Q      color.C      color^4      color^5  \n    110.340    -1910.288     -627.954     -171.960       21.678      -85.943  \n    color^6  \n    -49.986  \n\n\n[[13]]\n\nCall:\nlm(formula = mod_formula, data = df_filtered)\n\nCoefficients:\n(Intercept)        carat  \n      -2256         7757  \n\n\n[[14]]\n\nCall:\nlm(formula = mod_formula, data = df_filtered)\n\nCoefficients:\n(Intercept)        carat        cut.L        cut.Q        cut.C        cut^4  \n   -2701.47      7871.17      1239.78      -528.59       367.96        74.63  \n\n\n[[15]]\n\nCall:\nlm(formula = mod_formula, data = df_filtered)\n\nCoefficients:\n(Intercept)        carat        cut.L        cut.Q        cut.C        cut^4  \n  -3187.784     8472.316      713.686     -334.533      188.544        1.705  \n  clarity.L    clarity.Q    clarity.C    clarity^4    clarity^5    clarity^6  \n   4012.037    -1821.966      917.545     -429.913      257.052       26.916  \n  clarity^7  \n    186.767  \n\n\n[[16]]\n\nCall:\nlm(formula = mod_formula, data = df_filtered)\n\nCoefficients:\n(Intercept)        carat        cut.L        cut.Q        cut.C        cut^4  \n  -3710.814     8886.379      698.795     -327.726      180.624       -1.168  \n  clarity.L    clarity.Q    clarity.C    clarity^4    clarity^5    clarity^6  \n   4217.848    -1832.466      923.155     -361.872      216.522        2.114  \n  clarity^7      color.L      color.Q      color.C      color^4      color^5  \n    110.368    -1910.260     -627.961     -172.133       21.894      -86.104  \n    color^6  \n    -49.899"
  },
  {
    "objectID": "posts/lab8/index.html",
    "href": "posts/lab8/index.html",
    "title": "Lab 8",
    "section": "",
    "text": "Goal: Scrape information from https://www.cheese.com to obtain a dataset of characteristics about different cheeses, and gain deeper insight into your coding process. 🪤\nPart 1: Locate and examine the robots.txt file for this website. Summarize what you learn from it.\nThe robots.txt is located at https://www.cheese.com/robots.txt . This file is pretty blank, with only two lines. These two lines tell us that anyone can crawl (User-agent is set to *) and where the site map is located (https://www.cheese.com/sitemap.xml)\nPart 2: Learn about the html_attr() function from rvest. Describe how this function works with a small example.\nThe html_attr() function gets and returns a character vector containing the values of the specified attribute for an element of the HTML file (ex. href, class, id). This function requires two arguments; x, the HTML element (usually from read_html()), name, the name of the attribute to retrieve, like href, class, etc. There is a third optional argument, default, which is related to how non-existent attributes are handled, which defaults to NA_character, meaning NA is returned in these cases.\n\n# define url\ncheese_url &lt;- \"https://www.cheese.com\"\n\n# fetch html content \nhtml_doc_cheese &lt;- read_html(cheese_url) # note: in the real world, add error handling\n\n# select all &lt;a&gt; tags on page\nall_links_nodes &lt;- html_nodes(html_doc_cheese, \"a\")\n\n# use html_attr() to extract the 'href' attribute from each link\nall_hrefs_cheese &lt;- html_attr(all_links_nodes, \"href\")\n\nhead(all_hrefs_cheese, 10)\n\nPart 3: (Do this alongside Part 4 below.) I used ChatGPT to start the process of scraping cheese information with the following prompt:\n\nWrite R code using the rvest package that allows me to scrape cheese information from cheese.com.\n\nFully document your process of checking this code. Record any observations you make about where ChatGPT is useful / not useful.\n\n# load libraries\nlibrary(rvest)\nlibrary(dplyr)\n\n# define url\nurl &lt;- \"https://www.cheese.com/alphabetical\"\n\n# read html content from the page\nwebpage &lt;- read_html(url)\n\n# extract cheese names and urls\ncheese_data &lt;- webpage %&gt;%\n  html_nodes(\".cheese-item\") %&gt;%\n  html_nodes(\"a\") %&gt;%\n  html_attr(\"href\") %&gt;%\n  paste0(\"https://cheese.com\", .)\n\ncheese_names &lt;- webpage %&gt;%\n  html_nodes(\".cheese-item h3\") %&gt;%\n  html_text()\n\n# create df to store results\ncheese_df &lt;- data.frame(Name = cheese_names,\n                        URL = cheese_data,\n                        stringsAsFactors = FALSE)\n\nprint(cheese_df)\n\nNot useful:\nThere were many empty results. Both cheese_data (for URLs) and cheese_names (for cheese names) were empty character vectors. Because of this, the cheese_df dataframe was also empty.\nThe selectors were not specific enough. The CSS selectors .cheese-item and .cheese-item h3 suggested by ChatGPT were too generic/ did not accurately reflect the current structure of the cheese.com/alphabetical page. Websites frequently update their structure, and AI might be trained on old versions or make incorrect assumptions about common class names.\nUseful:\nChatGPT provided a basic template of rvest functions (read_html, html_nodes, html_attr, html_text) which was conceptually helpful for recalling the workflow.\nPart 4: Obtain the following information for all cheeses in the database:\n\ncheese name\nURL for the cheese’s webpage (e.g., https://www.cheese.com/gouda/)\nwhether or not the cheese has a picture (e.g., gouda has a picture, but bianco does not).\n\nTo be kind to the website owners, please add a 1 second pause between page queries. (Note that you can view 100 cheeses at a time.)\n\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(stringr)\n\n# defining url and pages to look at\nbase_url &lt;- \"https://www.cheese.com/alphabetical/?per_page=100\"\npage_numbers &lt;- 1:21\n\n# Helper function to extract text or attributes based on tag structure\nextract_info &lt;- function(page, outer_selector, inner_selector, attr = NULL) {\n  nodes &lt;- page %&gt;%\n    html_elements(outer_selector) %&gt;%\n    html_elements(inner_selector)\n  \n  if (!is.null(attr)) { #dealing with null values\n    html_attr(nodes, attr)\n  } else {\n    html_text(nodes)\n  }\n}\n\n# Function to scrape a single page\nscrape_cheese_page &lt;- function(page_number) {\n  full_url &lt;- paste0(base_url, \"&page=\", page_number)\n  page &lt;- read_html(full_url)\n\n  data.frame(\n    Name = extract_info(page, \"div.product-item\", \"h3\"), #cheese name\n    url = paste0(\"https://www.cheese.com\", extract_info(page, \"div.product-item\", \"h3 a\", \"href\")), #cheese url\n    whether = extract_info(page, \"div.product-item\", \"img\", \"class\"), #if there is image\n    stringsAsFactors = FALSE\n  )\n}\n\n# Map over all pages and bind results\ncheese_data &lt;- map_dfr(page_numbers, function(pg) {\n  result &lt;- scrape_cheese_page(pg)\n  Sys.sleep(1)  # delay to be nice\n  result\n})\n\nhead(cheese_data)\n\n                               Name\n1           2 Year Aged Cumin Gouda\n2            3-Cheese Italian Blend\n3 30 Month Aged Parmigiano Reggiano\n4           3yrs Aged Vintage Gouda\n5                        Aarewasser\n6                  Abbaye de Belloc\n                                                             url       whether\n1                https://www.cheese.com/2-year-aged-cumin-gouda/  image-exists\n2                 https://www.cheese.com/3-cheese-italian-blend/ image-missing\n3 https://www.cheese.com/30-month-aged-parmigiano-reggiano-150g/  image-exists\n4                https://www.cheese.com/3yrs-aged-vintage-gouda/  image-exists\n5                             https://www.cheese.com/aarewasser/  image-exists\n6                       https://www.cheese.com/abbaye-de-belloc/  image-exists\n\n\nPart 5: When you go to a particular cheese’s page (like gouda), you’ll see more detailed information about the cheese. For just 10 of the cheeses in the database, obtain the following detailed information:\n\nmilk information\ncountry of origin\nfamily\ntype\nflavour\n\n(Just 10 to avoid overtaxing the website! Continue adding a 1 second pause between page queries.)\n\nextract_text &lt;- function(page, selector) {\n  page %&gt;%\n    html_elements(selector) %&gt;%\n    html_text()\n}\n\n# Scrape cheese detail from a single page URL\nscrape_cheese_details &lt;- function(url) {\n  Sys.sleep(1)  # delay to be nice\n  \n  page &lt;- read_html(url)\n  \n  tibble(\n    family = extract_text(page, \".summary_family p\"),\n    milk = extract_text(page, \".summary_milk p\"),\n    country_of_origin = extract_text(page, \".summary_country p\"),\n    type = extract_text(page, \".summary_moisture_and_type p\"),\n    flavour = extract_text(page, \".summary_taste p\")\n  )\n}\n\ncheeses &lt;- c(\n    \"Gouda\", \"Colby\", \"Applewood\",\n    \"Vacherin\", \"Pecorino Romano\",\n    \"Cornish Blue\", \"Camembert\", \n    \"Stella Feta\", \"Dubliner\", \"Paneer\"\n  )\n# Select cheese URLs of interest\ncheese_urls &lt;- cheese_data %&gt;%\n  filter(Name %in% cheeses) %&gt;%\n  pull(url)\n\n# Map and combine all details into a single tibble\ndf_cheeses &lt;- map_dfr(cheese_urls, scrape_cheese_details)\n\n# cleaning df for readibility\ndf_cheeses &lt;- df_cheeses %&gt;%\n  mutate( # removing unnecessary labels in vars\n    family = str_remove(family, 'Family: '),\n    milk = str_remove(milk, 'Made from '),\n    country_of_origin = str_remove(country_of_origin, 'Country of origin: '),\n    type = str_remove(type, 'Type: '),\n    flavour = str_remove(flavour, \"Flavour: \")\n  )\n\nnames &lt;- cheese_data %&gt;%\n  filter(Name %in% cheeses) %&gt;%\n  select(Name)\n\ndf_cheeses &lt;- cbind(names, df_cheeses)\n\ndf_cheeses\n\n              Name    family\n1        Applewood   Cheddar\n2        Camembert Camembert\n3            Colby   Cheddar\n4     Cornish Blue      Blue\n5         Dubliner   Cheddar\n6            Gouda     Gouda\n7           Paneer   Cottage\n8  Pecorino Romano  Pecorino\n9      Stella Feta      Feta\n10        Vacherin      Brie\n                                                         milk\n1                                      pasteurized cow's milk\n2                                                  cow's milk\n3                                                  cow's milk\n4                                      pasteurized cow's milk\n5                                      pasteurized cow's milk\n6  pasteurized or unpasteurized cow's, goat's or sheep's milk\n7                   pasteurized cow's or water buffalo's milk\n8                                                sheep's milk\n9                                      pasteurized cow's milk\n10                                     pasteurized cow's milk\n        country_of_origin               type       flavour\n1                 England          semi-hard       smokey \n2                  France      soft, artisan         sweet\n3           United States          semi-hard         sweet\n4                 England semi-soft, artisan creamy, sweet\n5                 Ireland               hard  nutty, sweet\n6             Netherlands               hard full-flavored\n7    Bangladesh and India         fresh firm         milky\n8                   Italy               hard  salty, sharp\n9           United States      firm, artisan         tangy\n10 France and Switzerland      soft, artisan        smooth\n\n\nPart 6: Evaluate the code that you wrote in terms of efficiency. To what extent do your function(s) adhere to the principles for writing good functions? To what extent are your functions efficient? To what extent is your iteration of these functions efficient?\nThe functions we wrote follow the principles of good function design by being modular, clear, and reusable. Each function performs a single responsibility. For example, extract_info() and scrape_cheese_details() are each focused on one task. Using purrr::map_dfr() improves efficiency over for loops by combining iteration and row-binding in a memory-friendly way, avoiding repeated rbind() calls that can slow down execution. While Sys.sleep(1) adds intentional delay, it’s necessary for responsible scraping and to be nice to the website."
  },
  {
    "objectID": "posts/lab7/index.html",
    "href": "posts/lab7/index.html",
    "title": "Lab 7",
    "section": "",
    "text": "library(tidyverse)\nlibrary(httr)\nlibrary(tidyjson)\nlibrary(jsonlite)\nlibrary(leaflet)"
  },
  {
    "objectID": "posts/lab7/index.html#api-functions",
    "href": "posts/lab7/index.html#api-functions",
    "title": "Lab 7",
    "section": "API functions",
    "text": "API functions\n\n# function to get pass times for a single capital\n\nget_single_location_passes &lt;- function(lat, long) {\n  api_base_url &lt;- \"https://api.g7vrd.co.uk/v1/satellite-passes/25544\"\n  request_url &lt;- paste0(api_base_url, \"/\", lat, \"/\", long, \".json\")\n  \n  # get request\n  response &lt;- GET(request_url)\n  \n  # check if the request successful, if success get content\n  if (status_code(response) == 200) {\n    content &lt;- content(response, \"text\", encoding = \"UTF-8\")\n    pass_data &lt;- fromJSON(content)\n    \n    # extract time \n    if (length(pass_data$passes) &gt; 0 && \"tca\" %in% names(pass_data$passes)) {\n      # get first 3 TCA timestamps\n      return(head(pass_data$passes$tca, 3))\n    } else {\n      return(NULL) # if no passes found or tca field missing\n    }\n  } else {\n    warning(paste(\"API request failed for lat:\", lat, \"long:\", long, \"with status:\", status_code(response)))\n    return(NULL) # if request failed\n  }\n}\n\n# function to get pass times for capitals and create df\nget_all_capitals_passes &lt;- function(df) {\n  # empty list to store results\n  all_passes_list &lt;- list()\n  \n  # loop through each capital\n  for (i in 1:nrow(df)) {\n    capital_info &lt;- df[i, ]\n    \n    pass_times_vector &lt;- get_single_location_passes(lat = capital_info$lat, long = capital_info$long)\n    \n    # make tibble for the capital pass times\n    # three time columns, NA if less than 3 \n    current_passes_df &lt;- tibble(\n      state_abbr = capital_info$state,\n      capital_name = capital_info$capital,\n      lat = capital_info$lat,\n      long = capital_info$long,\n      time1 = if (length(pass_times_vector) &gt;= 1) pass_times_vector[1] else NA_character_,\n      time2 = if (length(pass_times_vector) &gt;= 2) pass_times_vector[2] else NA_character_,\n      time3 = if (length(pass_times_vector) &gt;= 3) pass_times_vector[3] else NA_character_\n    )\n    \n    all_passes_list[[i]] &lt;- current_passes_df\n    \n    # wait for 1 second between requests\n    Sys.sleep(1) \n  }\n  \n  # combine all tibbles\n  final_df &lt;- bind_rows(all_passes_list)\n  return(final_df)\n}\n\n\nPass Times Data\n\n# raw pass times for all capitals\niss_pass_data_raw &lt;- get_all_capitals_passes(capitals)\n\n# convert times and sort\niss_pass_data_processed &lt;- iss_pass_data_raw |&gt;\n  # convert UTC to datetime - help from AI\n  mutate(\n    time1_dt = ymd_hms(time1, tz = \"UTC\"),\n    time2_dt = ymd_hms(time2, tz = \"UTC\"),\n    time3_dt = ymd_hms(time3, tz = \"UTC\")\n  ) |&gt;\n  # arrange by first pass time\n  arrange(time1_dt) |&gt;\n  # remove rows where time 1 is NA\n  filter(!is.na(time1_dt))"
  },
  {
    "objectID": "posts/lab7/index.html#mapping-the-data-and-drawing-the-iss-route",
    "href": "posts/lab7/index.html#mapping-the-data-and-drawing-the-iss-route",
    "title": "Lab 7",
    "section": "2-5. Mapping the Data and Drawing the ISS Route",
    "text": "2-5. Mapping the Data and Drawing the ISS Route\n\nDefine Custom Icon\n\nsatellite_icon &lt;- makeIcon(\n  iconUrl = \"https://png.pngtree.com/png-clipart/20230111/original/pngtree-rocket-icon-vector-png-image_8902705.png\",\n  iconWidth = 25, \n  iconHeight = 25,\n  iconAnchorX = 12, \n  iconAnchorY = 12  \n)\n\n\n\nCreate the Leaflet Map\nWe construct the map layer by layer.\n\n# ensure there is data for plot\nif (nrow(iss_pass_data_processed) &gt; 0) {\n  \n  # format times (local time zone for readability) for labels etc\n  \n  map_data &lt;- iss_pass_data_processed |&gt;\n    mutate(\n      time1_display = format(time1_dt, \"%Y-%m-%d %H:%M:%S UTC\"),\n      time2_display = format(time2_dt, \"%Y-%m-%d %H:%M:%S UTC\"),\n      time3_display = format(time3_dt, \"%Y-%m-%d %H:%M:%S UTC\"),\n      \n      # handle NA times in display strings\n      time2_display = ifelse(is.na(time2_dt), \"N/A\", time2_display),\n      time3_display = ifelse(is.na(time3_dt), \"N/A\", time3_display)\n    )\n\n  # hover labels\n  hover_labels &lt;- paste0(\n    \"&lt;strong&gt;Capital:&lt;/strong&gt; \", map_data$capital_name, \"&lt;br&gt;\",\n    \"&lt;strong&gt;State:&lt;/strong&gt; \", map_data$state_abbr, \"&lt;br&gt;\",\n    \"&lt;strong&gt;Soonest Pass:&lt;/strong&gt; \", map_data$time1_display\n  ) |&gt; lapply(htmltools::HTML) # lapply with HTML for proper rendering\n\n  # click popups\n  click_popups &lt;- paste0(\n    \"&lt;strong&gt;Capital:&lt;/strong&gt; \", map_data$capital_name, \" (\", map_data$state_abbr, \")&lt;br&gt;&lt;br&gt;\",\n    \"&lt;strong&gt;Predicted Pass Times (UTC):&lt;/strong&gt;&lt;br&gt;\",\n    \"1. \", map_data$time1_display, \"&lt;br&gt;\",\n    \"2. \", map_data$time2_display, \"&lt;br&gt;\",\n    \"3. \", map_data$time3_display\n  ) |&gt; lapply(htmltools::HTML)\n\n  # create the map\n  iss_map &lt;- leaflet(data = map_data) |&gt;\n    addTiles(group = \"OSM (Default)\") |&gt; # add default OpenStreetMap map tiles\n    addProviderTiles(providers$CartoDB.Positron, group = \"CartoDB Positron\") |&gt;\n    addProviderTiles(providers$Esri.WorldImagery, group = \"Esri World Imagery\") |&gt;\n    \n    # markers for each state capital\n    addMarkers(\n      lng = ~long, \n      lat = ~lat,\n      icon = satellite_icon,\n      label = hover_labels,\n      popup = click_popups,\n      group = \"State Capitals\"\n    ) |&gt;\n    \n    #  polylines connecting capitals in order of first pass time\n    addPolylines(\n      lng = ~long,\n      lat = ~lat,\n      color = \"#E6007E\", \n      weight = 3,\n      opacity = 0.8,\n      dashArray = \"5, 5\", # dashed line\n      group = \"ISS Pass Order Path\"\n    ) |&gt;\n    \n    #  layer controls to toggle layers\n    addLayersControl(\n      baseGroups = c(\"OSM (Default)\", \"CartoDB Positron\", \"Esri World Imagery\"),\n      overlayGroups = c(\"State Capitals\", \"ISS Pass Order Path\"),\n      options = layersControlOptions(collapsed = FALSE)\n    ) |&gt;\n    \n    # legend for the polyline\n    addLegend(\n        position = \"bottomright\",\n        colors = \"#E6007E\",\n        labels = \"ISS Pass Order Path\",\n        title = \"Map Features\"\n    )\n\n  # display the map\n  iss_map\n\n} else {\n  print(\"No ISS pass data available to map. Check API calls or data processing steps.\")\n}"
  }
]